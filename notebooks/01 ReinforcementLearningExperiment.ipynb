{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd97f7c3-a9bf-4d6a-9377-9422535e799a",
   "metadata": {},
   "source": [
    "# RL Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "633518c9-f30a-4242-85c5-a9291144a1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pygame\n",
      "  Downloading pygame-2.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.7 MB 5.6 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pygame\n",
      "Successfully installed pygame-2.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install pygame\n",
    "# pip install gym==0.26.3\n",
    "# pip install git+https://github.com/carlosluis/stable-baselines3@fix_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6581e60-fafa-4701-8939-89035ca7f6c5",
   "metadata": {},
   "source": [
    "# XY. Create custom environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f3bd234-deba-422b-a9fe-5c01742ad323",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import gym\n",
    "## from gym import spaces\n",
    "## import pygame\n",
    "## import numpy as np\n",
    "## import stable_baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01d7053e-3c87-4f89-8b7b-6f757f053ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### class TestEnv(gym.Env):\n",
    "###     metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
    "### \n",
    "###     def __init__(self, render_mode=None, size=5):\n",
    "###         self.size = size  # The size of the square grid\n",
    "###         self.window_size = 512  # The size of the PyGame window\n",
    "### \n",
    "###         # Observations are dictionaries with the agent's and the target's location.\n",
    "###         # Each location is encoded as an element of {0, ..., `size`}^2, i.e. MultiDiscrete([size, size]).\n",
    "###         self.observation_space = spaces.Dict(\n",
    "###             {\n",
    "###                 \"agent\": spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
    "###                 \"target\": spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
    "###             }\n",
    "###         )\n",
    "### \n",
    "###         # We have 4 actions, corresponding to \"right\", \"up\", \"left\", \"down\"\n",
    "###         self.action_space = spaces.Discrete(4)\n",
    "### \n",
    "###         \"\"\"\n",
    "###         The following dictionary maps abstract actions from `self.action_space` to \n",
    "###         the direction we will walk in if that action is taken.\n",
    "###         I.e. 0 corresponds to \"right\", 1 to \"up\" etc.\n",
    "###         \"\"\"\n",
    "###         self._action_to_direction = {\n",
    "###             0: np.array([1, 0]),\n",
    "###             1: np.array([0, 1]),\n",
    "###             2: np.array([-1, 0]),\n",
    "###             3: np.array([0, -1]),\n",
    "###         }\n",
    "### \n",
    "###         assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "###         self.render_mode = render_mode\n",
    "### \n",
    "###         \"\"\"\n",
    "###         If human-rendering is used, `self.window` will be a reference\n",
    "###         to the window that we draw to. `self.clock` will be a clock that is used\n",
    "###         to ensure that the environment is rendered at the correct framerate in\n",
    "###         human-mode. They will remain `None` until human-mode is used for the\n",
    "###         first time.\n",
    "###         \"\"\"\n",
    "###         self.window = None\n",
    "###         self.clock = None\n",
    "###     \n",
    "###     def _get_obs(self):\n",
    "###         return {\"agent\": self._agent_location, \"target\": self._target_location}\n",
    "###     \n",
    "###     def _get_info(self):\n",
    "###         return {\"distance\": np.linalg.norm(self._agent_location - self._target_location, ord=1)}\n",
    "### \n",
    "###     def reset(self, seed=None, options=None):\n",
    "###         # We need the following line to seed self.np_random\n",
    "###         super().reset(seed=seed)\n",
    "### \n",
    "###         # Choose the agent's location uniformly at random\n",
    "###         self._agent_location = self.np_random.integers(0, self.size, size=2, dtype=int)\n",
    "### \n",
    "###         # We will sample the target's location randomly until it does not coincide with the agent's location\n",
    "###         self._target_location = self._agent_location\n",
    "###         while np.array_equal(self._target_location, self._agent_location):\n",
    "###             self._target_location = self.np_random.integers(\n",
    "###                 0, self.size, size=2, dtype=int\n",
    "###             )\n",
    "### \n",
    "###         observation = self._get_obs()\n",
    "###         info = self._get_info()\n",
    "### \n",
    "###         if self.render_mode == \"human\":\n",
    "###             self._render_frame()\n",
    "### \n",
    "###         return observation, info\n",
    "###     \n",
    "###     def step(self, action):\n",
    "###         # Map the action (element of {0,1,2,3}) to the direction we walk in\n",
    "###         direction = self._action_to_direction[action]\n",
    "###         # We use `np.clip` to make sure we don't leave the grid\n",
    "###         self._agent_location = np.clip(\n",
    "###             self._agent_location + direction, 0, self.size - 1\n",
    "###         )\n",
    "###         # An episode is done iff the agent has reached the target\n",
    "###         terminated = np.array_equal(self._agent_location, self._target_location)\n",
    "###         reward = 1 if terminated else 0  # Binary sparse rewards\n",
    "###         observation = self._get_obs()\n",
    "###         info = self._get_info()\n",
    "### \n",
    "###         if self.render_mode == \"human\":\n",
    "###             self._render_frame()\n",
    "### \n",
    "###         return observation, reward, terminated, False, info\n",
    "###     \n",
    "###     def render(self):\n",
    "###         if self.render_mode == \"rgb_array\":\n",
    "###             return self._render_frame()\n",
    "### \n",
    "###     def _render_frame(self):\n",
    "###         if self.window is None and self.render_mode == \"human\":\n",
    "###             pygame.init()\n",
    "###             pygame.display.init()\n",
    "###             self.window = pygame.display.set_mode((self.window_size, self.window_size))\n",
    "###         if self.clock is None and self.render_mode == \"human\":\n",
    "###             self.clock = pygame.time.Clock()\n",
    "### \n",
    "###         canvas = pygame.Surface((self.window_size, self.window_size))\n",
    "###         canvas.fill((255, 255, 255))\n",
    "###         pix_square_size = (\n",
    "###             self.window_size / self.size\n",
    "###         )  # The size of a single grid square in pixels\n",
    "### \n",
    "###         # First we draw the target\n",
    "###         pygame.draw.rect(\n",
    "###             canvas,\n",
    "###             (255, 0, 0),\n",
    "###             pygame.Rect(\n",
    "###                 pix_square_size * self._target_location,\n",
    "###                 (pix_square_size, pix_square_size),\n",
    "###             ),\n",
    "###         )\n",
    "###         # Now we draw the agent\n",
    "###         pygame.draw.circle(\n",
    "###             canvas,\n",
    "###             (0, 0, 255),\n",
    "###             (self._agent_location + 0.5) * pix_square_size,\n",
    "###             pix_square_size / 3,\n",
    "###         )\n",
    "### \n",
    "###         # Finally, add some gridlines\n",
    "###         for x in range(self.size + 1):\n",
    "###             pygame.draw.line(\n",
    "###                 canvas,\n",
    "###                 0,\n",
    "###                 (0, pix_square_size * x),\n",
    "###                 (self.window_size, pix_square_size * x),\n",
    "###                 width=3,\n",
    "###             )\n",
    "###             pygame.draw.line(\n",
    "###                 canvas,\n",
    "###                 0,\n",
    "###                 (pix_square_size * x, 0),\n",
    "###                 (pix_square_size * x, self.window_size),\n",
    "###                 width=3,\n",
    "###             )\n",
    "### \n",
    "###         if self.render_mode == \"human\":\n",
    "###             # The following line copies our drawings from `canvas` to the visible window\n",
    "###             self.window.blit(canvas, canvas.get_rect())\n",
    "###             pygame.event.pump()\n",
    "###             pygame.display.update()\n",
    "### \n",
    "###             # We need to ensure that human-rendering occurs at the predefined framerate.\n",
    "###             # The following line will automatically add a delay to keep the framerate stable.\n",
    "###             self.clock.tick(self.metadata[\"render_fps\"])\n",
    "###         else:  # rgb_array\n",
    "###             return np.transpose(\n",
    "###                 np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
    "###             )\n",
    "###         \n",
    "###         def close(self):\n",
    "###             if self.window is not None:\n",
    "###                 pygame.display.quit()\n",
    "###                 pygame.quit()\n",
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5990958c-88eb-4208-a1c4-278d13234f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### from gym.envs.registration import register\n",
    "### \n",
    "### register(\n",
    "###     id='gym_examples/TestEnv-v0',\n",
    "###     entry_point='gym_examples.envs:TestEnv',\n",
    "###     max_episode_steps=300,\n",
    "### )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69635b70-e731-49ad-8a90-7d2a89f5d204",
   "metadata": {},
   "outputs": [],
   "source": [
    "### env = gym.make('gym_examples/TestEnv-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07139d66-407a-4bfb-bf53-8a0c5276b01f",
   "metadata": {},
   "source": [
    "# 2. Gym Example \n",
    "based on https://www.gymlibrary.dev/content/environment_creation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d18c8f0-2dc0-4902-9127-4ac39f801dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import Env\n",
    "from gym import spaces\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import random\n",
    "import pygame\n",
    "import stable_baselines3\n",
    "from stable_baselines3 import PPO, A2C, SAC, TD3, DQN\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e622e9a2-3abb-4a79-9e8b-454d8986f9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorldEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
    "\n",
    "    def __init__(self, render_mode=None, size=5):\n",
    "        self.size = size  # The size of the square grid\n",
    "        self.window_size = 512  # The size of the PyGame window\n",
    "\n",
    "        # Observations are dictionaries with the agent's and the target's location.\n",
    "        # Each location is encoded as an element of {0, ..., `size`}^2, i.e. MultiDiscrete([size, size]).\n",
    "        self.observation_space = spaces.Dict(\n",
    "            {\n",
    "                \"agent\": spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
    "                \"target\": spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # We have 4 actions, corresponding to \"right\", \"up\", \"left\", \"down\"\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        \"\"\"\n",
    "        The following dictionary maps abstract actions from `self.action_space` to \n",
    "        the direction we will walk in if that action is taken.\n",
    "        I.e. 0 corresponds to \"right\", 1 to \"up\" etc.\n",
    "        \"\"\"\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([1, 0]),\n",
    "            1: np.array([0, 1]),\n",
    "            2: np.array([-1, 0]),\n",
    "            3: np.array([0, -1]),\n",
    "        }\n",
    "\n",
    "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        \"\"\"\n",
    "        If human-rendering is used, `self.window` will be a reference\n",
    "        to the window that we draw to. `self.clock` will be a clock that is used\n",
    "        to ensure that the environment is rendered at the correct framerate in\n",
    "        human-mode. They will remain `None` until human-mode is used for the\n",
    "        first time.\n",
    "        \"\"\"\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "        \n",
    "    def _get_obs(self):\n",
    "        return {\"agent\": self._agent_location, \"target\": self._target_location}\n",
    "\n",
    "    def _get_info(self):\n",
    "        return {\"distance\": np.linalg.norm(self._agent_location - self._target_location, ord=1)}\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # We need the following line to seed self.np_random\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Choose the agent's location uniformly at random\n",
    "        self._agent_location = self.np_random.integers(0, self.size, size=2, dtype=int)\n",
    "\n",
    "        # We will sample the target's location randomly until it does not coincide with the agent's location\n",
    "        self._target_location = self._agent_location\n",
    "        while np.array_equal(self._target_location, self._agent_location):\n",
    "            self._target_location = self.np_random.integers(\n",
    "                0, self.size, size=2, dtype=int\n",
    "            )\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        # Map the action (element of {0,1,2,3}) to the direction we walk in\n",
    "        direction = self._action_to_direction[action]\n",
    "        # We use `np.clip` to make sure we don't leave the grid\n",
    "        self._agent_location = np.clip(\n",
    "            self._agent_location + direction, 0, self.size - 1\n",
    "        )\n",
    "        # An episode is done iff the agent has reached the target\n",
    "        terminated = np.array_equal(self._agent_location, self._target_location)\n",
    "        distance = np.linalg.norm(self._agent_location - self._target_location, ord=1)\n",
    "        reward = 1000 if terminated else - distance  # Binary sparse rewards\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, reward, terminated, False, info\n",
    "    \n",
    "    def render(self):\n",
    "        if self.render_mode == \"rgb_array\":\n",
    "            return self._render_frame()\n",
    "\n",
    "    def _render_frame(self):\n",
    "        if self.window is None and self.render_mode == \"human\":\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.window = pygame.display.set_mode((self.window_size, self.window_size))\n",
    "        if self.clock is None and self.render_mode == \"human\":\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
    "        canvas.fill((255, 255, 255))\n",
    "        pix_square_size = (\n",
    "            self.window_size / self.size\n",
    "        )  # The size of a single grid square in pixels\n",
    "\n",
    "        # First we draw the target\n",
    "        pygame.draw.rect(\n",
    "            canvas,\n",
    "            (255, 0, 0),\n",
    "            pygame.Rect(\n",
    "                pix_square_size * self._target_location,\n",
    "                (pix_square_size, pix_square_size),\n",
    "            ),\n",
    "        )\n",
    "        # Now we draw the agent\n",
    "        pygame.draw.circle(\n",
    "            canvas,\n",
    "            (0, 0, 255),\n",
    "            (self._agent_location + 0.5) * pix_square_size,\n",
    "            pix_square_size / 3,\n",
    "        )\n",
    "\n",
    "        # Finally, add some gridlines\n",
    "        for x in range(self.size + 1):\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (0, pix_square_size * x),\n",
    "                (self.window_size, pix_square_size * x),\n",
    "                width=3,\n",
    "            )\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (pix_square_size * x, 0),\n",
    "                (pix_square_size * x, self.window_size),\n",
    "                width=3,\n",
    "            )\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            # The following line copies our drawings from `canvas` to the visible window\n",
    "            self.window.blit(canvas, canvas.get_rect())\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "\n",
    "            # We need to ensure that human-rendering occurs at the predefined framerate.\n",
    "            # The following line will automatically add a delay to keep the framerate stable.\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "        else:  # rgb_array\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
    "            )\n",
    "\n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c21937c8-d73d-43b1-86f6-5def194bdb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "Genv = GridWorldEnv(size= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b06db65-1e99-47b0-99b3-6657bd855632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'agent': array([7, 3]), 'target': array([2, 9])}, {'distance': 11.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Genv.reset(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abc71664-48e3-42e1-80f9-229e7c3a694a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Genv.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43995295-2be6-499c-b16b-b499478b8bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('agent', array([1, 5])), ('target', array([2, 4]))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Genv.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3fb2528-c8a3-43e9-9809-974ebe08d5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 | Steps: 10 | State: [4 9] | Distance: 6.0 | Score: -37.0\n",
      "Episode: 1 | Steps: 10 | State: [2 7] | Distance: 7.0 | Score: -80.0\n",
      "Episode: 2 | Steps: 10 | State: [9 8] | Distance: 9.0 | Score: -88.0\n",
      "Episode: 3 | Steps: 4 | State: [1 7] | Distance: 0.0 | Score: 994.0\n",
      "Episode: 4 | Steps: 10 | State: [5 5] | Distance: 8.0 | Score: -53.0\n",
      "Episode: 5 | Steps: 10 | State: [8 9] | Distance: 13.0 | Score: -135.0\n",
      "Episode: 6 | Steps: 1 | State: [7 0] | Distance: 0.0 | Score: 1000\n",
      "Episode: 7 | Steps: 8 | State: [9 7] | Distance: 0.0 | Score: 976.0\n",
      "Episode: 8 | Steps: 10 | State: [5 0] | Distance: 2.0 | Score: -32.0\n",
      "Episode: 9 | Steps: 10 | State: [7 2] | Distance: 6.0 | Score: -79.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "for episode in range(episodes):\n",
    "    state = Genv.reset()\n",
    "    terminated = False\n",
    "    score = 0\n",
    "    n_state = 0\n",
    "    \n",
    "    while not terminated:\n",
    "        action = Genv.action_space.sample()\n",
    "        # observation, reward, terminated, False, info\n",
    "        observation, reward, terminated, UnclearBool, distance = Genv.step(action)\n",
    "        score += reward\n",
    "        n_state += 1\n",
    "        if n_state == episodes:\n",
    "            terminated = True\n",
    "    print('Episode: {} | Steps: {} | State: {} | Distance: {} | Score: {}'.format(episode, n_state, observation['agent'], \n",
    "                                                                    distance['distance'], score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8b2f5cf-4b04-43b4-bee1-37cf2eceb17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "budget_pendulum = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c64acf60-4411-490b-bf84-ad97d3e85574",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_model = PPO(\"MultiInputPolicy\", Genv, verbose=0).learn(budget_pendulum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "87051cf9-47e3-4a9c-846a-51fa216eae22",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = Genv.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "041ef76b-a085-46e6-8eec-6ccfd2f65405",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mppo_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/stable_baselines3/common/base_class.py:539\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    521\u001b[0m     observation: Union[np\u001b[38;5;241m.\u001b[39mndarray, Dict[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    524\u001b[0m     deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    525\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, Optional[Tuple[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;124;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/stable_baselines3/common/policies.py:328\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# TODO (GH/1): add support for RNN policies\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# if state is None:\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m#     state = self.initial_state\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# if episode_start is None:\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;66;03m#     episode_start = [False for _ in range(self.n_envs)]\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;66;03m# Switch to eval mode (this affects batch norm / dropout)\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_training_mode(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 328\u001b[0m observation, vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    331\u001b[0m     actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict(observation, deterministic\u001b[38;5;241m=\u001b[39mdeterministic)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/stable_baselines3/common/policies.py:243\u001b[0m, in \u001b[0;36mBaseModel.obs_to_tensor\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    239\u001b[0m     observation \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(observation)\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m# Dict obs need to be handled separately\u001b[39;00m\n\u001b[0;32m--> 243\u001b[0m     vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[43mis_vectorized_observation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;66;03m# Add batch dimension if needed\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     observation \u001b[38;5;241m=\u001b[39m observation\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/stable_baselines3/common/utils.py:399\u001b[0m, in \u001b[0;36mis_vectorized_observation\u001b[0;34m(observation, observation_space)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m space_type, is_vec_obs_func \u001b[38;5;129;01min\u001b[39;00m is_vec_obs_func_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation_space, space_type):\n\u001b[0;32m--> 399\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mis_vec_obs_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservation_space\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# for-else happens if no break is called\u001b[39;00m\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: Cannot determine if the observation is vectorized with the space type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobservation_space\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/stable_baselines3/common/utils.py:349\u001b[0m, in \u001b[0;36mis_vectorized_dict_observation\u001b[0;34m(observation, observation_space)\u001b[0m\n\u001b[1;32m    345\u001b[0m all_non_vectorized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, subspace \u001b[38;5;129;01min\u001b[39;00m observation_space\u001b[38;5;241m.\u001b[39mspaces\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;66;03m# This fails when the observation is not vectorized\u001b[39;00m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;66;03m# or when it has the wrong shape\u001b[39;00m\n\u001b[0;32m--> 349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mobservation\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m subspace\u001b[38;5;241m.\u001b[39mshape:\n\u001b[1;32m    350\u001b[0m         all_non_vectorized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    351\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "ppo_model.predict(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea3b28e1-8e01-4743-ac16-1f97db9718b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     action, _states \u001b[38;5;241m=\u001b[39m \u001b[43mppo_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     obs, rewards, dones, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/stable_baselines3/common/base_class.py:539\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    521\u001b[0m     observation: Union[np\u001b[38;5;241m.\u001b[39mndarray, Dict[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    524\u001b[0m     deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    525\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, Optional[Tuple[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;124;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/stable_baselines3/common/policies.py:328\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# TODO (GH/1): add support for RNN policies\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# if state is None:\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m#     state = self.initial_state\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# if episode_start is None:\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;66;03m#     episode_start = [False for _ in range(self.n_envs)]\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;66;03m# Switch to eval mode (this affects batch norm / dropout)\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_training_mode(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 328\u001b[0m observation, vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    331\u001b[0m     actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict(observation, deterministic\u001b[38;5;241m=\u001b[39mdeterministic)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/stable_baselines3/common/policies.py:243\u001b[0m, in \u001b[0;36mBaseModel.obs_to_tensor\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    239\u001b[0m     observation \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(observation)\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m# Dict obs need to be handled separately\u001b[39;00m\n\u001b[0;32m--> 243\u001b[0m     vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[43mis_vectorized_observation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;66;03m# Add batch dimension if needed\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     observation \u001b[38;5;241m=\u001b[39m observation\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/stable_baselines3/common/utils.py:399\u001b[0m, in \u001b[0;36mis_vectorized_observation\u001b[0;34m(observation, observation_space)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m space_type, is_vec_obs_func \u001b[38;5;129;01min\u001b[39;00m is_vec_obs_func_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation_space, space_type):\n\u001b[0;32m--> 399\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mis_vec_obs_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservation_space\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# for-else happens if no break is called\u001b[39;00m\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: Cannot determine if the observation is vectorized with the space type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobservation_space\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/stable_baselines3/common/utils.py:349\u001b[0m, in \u001b[0;36mis_vectorized_dict_observation\u001b[0;34m(observation, observation_space)\u001b[0m\n\u001b[1;32m    345\u001b[0m all_non_vectorized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, subspace \u001b[38;5;129;01min\u001b[39;00m observation_space\u001b[38;5;241m.\u001b[39mspaces\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;66;03m# This fails when the observation is not vectorized\u001b[39;00m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;66;03m# or when it has the wrong shape\u001b[39;00m\n\u001b[0;32m--> 349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mobservation\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m subspace\u001b[38;5;241m.\u001b[39mshape:\n\u001b[1;32m    350\u001b[0m         all_non_vectorized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    351\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    action, _states = ppo_model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33792cd0-16e4-4a42-a23c-2211d3a71a09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ed579d-53cf-48b6-bf43-7bd89b8345d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a5da6a-da27-42ed-8371-049828a65e54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0393ff2-be9c-4d66-884f-24465f4dc891",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82cf54e2-63d1-4def-9612-ac5797301594",
   "metadata": {},
   "source": [
    "# 3 Shower Example\n",
    "based on the video: https://www.youtube.com/watch?v=bD6V3rcr_54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bdd0a0-1f03-4608-a543-6f155dde1418",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewEnv(Env):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # actions\n",
    "        self.action_space = Discrete(3)\n",
    "        # state\n",
    "        self.state = 38 + random.randint(-3,3)\n",
    "        # observation\n",
    "        self.observation_space = Box(low = np.array([0]), high = np.array([100]))\n",
    "        # episodes\n",
    "        self.episodes_length = 60\n",
    "        \n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        self.state += action - 1\n",
    "        self.episodes_length -= 1\n",
    "        if self.state >=37 and self.state <= 39:\n",
    "            reward += 1\n",
    "        else:\n",
    "            reward -= 1\n",
    "        # check if the time is up \n",
    "        if self.episodes_length <= 0:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        # Apply temperature noise\n",
    "        self.state += random.randint(-1,1)\n",
    "        \n",
    "        # placeholder for info\n",
    "        info = spaces.Box(low=-1.0, high=2.0, shape=(3, 4), dtype=np.float32)\n",
    "        \n",
    "        return self.state, reward, done, info\n",
    "        \n",
    "    def render(self):\n",
    "        # implement visualization\n",
    "        pass\n",
    "    def reset(self):\n",
    "        # reset\n",
    "        self.state = 38 + random.randint(-3,3)\n",
    "        self.episodes_length = 60\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eaf8ed9f-df8d-4a1a-8ed8-b843cf0c7b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = NewEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c136ec05-ab57-4a37-b470-cce196eb70dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([60.771236], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a293cc2c-3f76-465e-89c0-7eba47d81553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41,\n",
       " -1,\n",
       " False,\n",
       " Box([[-1. -1. -1. -1.]\n",
       "  [-1. -1. -1. -1.]\n",
       "  [-1. -1. -1. -1.]], [[2. 2. 2. 2.]\n",
       "  [2. 2. 2. 2.]\n",
       "  [2. 2. 2. 2.]], (3, 4), float32))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6275c5a7-0d78-4461-8292-e153e7545c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 | State: 35 | Action: 0 | Score: 2\n",
      "Episode: 1 | State: 32 | Action: 2 | Score: -54\n",
      "Episode: 2 | State: 36 | Action: 1 | Score: 4\n",
      "Episode: 3 | State: 41 | Action: 1 | Score: 10\n",
      "Episode: 4 | State: 30 | Action: 0 | Score: -30\n",
      "Episode: 5 | State: 30 | Action: 2 | Score: -52\n",
      "Episode: 6 | State: 38 | Action: 1 | Score: -36\n",
      "Episode: 7 | State: 39 | Action: 2 | Score: -34\n",
      "Episode: 8 | State: 36 | Action: 1 | Score: -38\n",
      "Episode: 9 | State: 39 | Action: 2 | Score: -30\n"
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    print('Episode: {} | State: {} | Action: {} | Score: {}'.format(episode, n_state, action, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4ebd13-c6a2-406b-8082-e59be728a7ae",
   "metadata": {},
   "source": [
    "## RL on \"shower environment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f982edc-fa29-45fd-849e-3eb6208c9192",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO, A2C, SAC, TD3, DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e7ed3e04-f1d5-4f7a-bf7b-63f20ee61e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install stable_baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6523fd3b-089f-4c95-bb32-a7f3d8f41a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4000 training timesteps\n",
    "budget_pendulum = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c209b3dc-ffd0-44dd-b226-5fc4b6182d68",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Box' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ppo_model \u001b[38;5;241m=\u001b[39m \u001b[43mPPO\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMlpPolicy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbudget_pendulum\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:307\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    299\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    304\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    305\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py:248\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    244\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 248\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m continue_training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py:184\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callback\u001b[38;5;241m.\u001b[39mon_step() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_info_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m n_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space, spaces\u001b[38;5;241m.\u001b[39mDiscrete):\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;66;03m# Reshape in case of discrete action\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/stable_baselines3/common/base_class.py:434\u001b[0m, in \u001b[0;36mBaseAlgorithm._update_info_buffer\u001b[0;34m(self, infos, dones)\u001b[0m\n\u001b[1;32m    432\u001b[0m     dones \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28;01mFalse\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(infos))\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, info \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(infos):\n\u001b[0;32m--> 434\u001b[0m     maybe_ep_info \u001b[38;5;241m=\u001b[39m \u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    435\u001b[0m     maybe_is_success \u001b[38;5;241m=\u001b[39m info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_success\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m maybe_ep_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Box' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "ppo_model = PPO(\"MlpPolicy\", env, verbose=0).learn(budget_pendulum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c86744-3a78-48fa-a1a2-a464999a22ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_model.learn(total_timesteps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba79fad-b308-4e57-9d8a-7faf1a5a3715",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00f2109-299a-448d-93b0-6f14ddb3d77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    action, _states = ppo_model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bb72c9b5-a4b6-4eb5-89e4-5ddb095ce392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37, 1, False, {})"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "183fa867-b692-4a5a-b464-54ef3c38271d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Box2D'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [103]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLunarLander-v2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhuman\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      5\u001b[0m observation, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/gym/envs/registration.py:581\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    578\u001b[0m     env_creator \u001b[38;5;241m=\u001b[39m spec_\u001b[38;5;241m.\u001b[39mentry_point\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;66;03m# Assume it's a string\u001b[39;00m\n\u001b[0;32m--> 581\u001b[0m     env_creator \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentry_point\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    583\u001b[0m mode \u001b[38;5;241m=\u001b[39m _kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrender_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    584\u001b[0m apply_human_rendering \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/gym/envs/registration.py:61\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m\"\"\"Loads an environment with name and returns an environment creation function\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m    Calls the environment constructor\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     60\u001b[0m mod_name, attr_name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 61\u001b[0m mod \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(mod, attr_name)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:850\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:228\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/gym/envs/box2d/lunar_lander.py:33\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mBox2D\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mBox2D\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mb2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     35\u001b[0m     edgeShape,\n\u001b[1;32m     36\u001b[0m     circleShape,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m     contactListener,\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Box2D'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "env.action_space.seed(42)\n",
    "\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "for _ in range(1000):\n",
    "    observation, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
