{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd97f7c3-a9bf-4d6a-9377-9422535e799a",
   "metadata": {},
   "source": [
    "# RL Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "633518c9-f30a-4242-85c5-a9291144a1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pygame\n",
      "  Downloading pygame-2.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.7 MB 5.6 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pygame\n",
      "Successfully installed pygame-2.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install pygame\n",
    "# pip install gym==0.26.3\n",
    "# pip install git+https://github.com/carlosluis/stable-baselines3@fix_tests # compatible with gym 0.24+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6581e60-fafa-4701-8939-89035ca7f6c5",
   "metadata": {},
   "source": [
    "# XY. Create custom environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f3bd234-deba-422b-a9fe-5c01742ad323",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## import gym\n",
    "## from gym import spaces\n",
    "## import pygame\n",
    "## import numpy as np\n",
    "## import stable_baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01d7053e-3c87-4f89-8b7b-6f757f053ed5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### class TestEnv(gym.Env):\n",
    "###     metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
    "### \n",
    "###     def __init__(self, render_mode=None, size=5):\n",
    "###         self.size = size  # The size of the square grid\n",
    "###         self.window_size = 512  # The size of the PyGame window\n",
    "### \n",
    "###         # Observations are dictionaries with the agent's and the target's location.\n",
    "###         # Each location is encoded as an element of {0, ..., `size`}^2, i.e. MultiDiscrete([size, size]).\n",
    "###         self.observation_space = spaces.Dict(\n",
    "###             {\n",
    "###                 \"agent\": spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
    "###                 \"target\": spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
    "###             }\n",
    "###         )\n",
    "### \n",
    "###         # We have 4 actions, corresponding to \"right\", \"up\", \"left\", \"down\"\n",
    "###         self.action_space = spaces.Discrete(4)\n",
    "### \n",
    "###         \"\"\"\n",
    "###         The following dictionary maps abstract actions from `self.action_space` to \n",
    "###         the direction we will walk in if that action is taken.\n",
    "###         I.e. 0 corresponds to \"right\", 1 to \"up\" etc.\n",
    "###         \"\"\"\n",
    "###         self._action_to_direction = {\n",
    "###             0: np.array([1, 0]),\n",
    "###             1: np.array([0, 1]),\n",
    "###             2: np.array([-1, 0]),\n",
    "###             3: np.array([0, -1]),\n",
    "###         }\n",
    "### \n",
    "###         assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "###         self.render_mode = render_mode\n",
    "### \n",
    "###         \"\"\"\n",
    "###         If human-rendering is used, `self.window` will be a reference\n",
    "###         to the window that we draw to. `self.clock` will be a clock that is used\n",
    "###         to ensure that the environment is rendered at the correct framerate in\n",
    "###         human-mode. They will remain `None` until human-mode is used for the\n",
    "###         first time.\n",
    "###         \"\"\"\n",
    "###         self.window = None\n",
    "###         self.clock = None\n",
    "###     \n",
    "###     def _get_obs(self):\n",
    "###         return {\"agent\": self._agent_location, \"target\": self._target_location}\n",
    "###     \n",
    "###     def _get_info(self):\n",
    "###         return {\"distance\": np.linalg.norm(self._agent_location - self._target_location, ord=1)}\n",
    "### \n",
    "###     def reset(self, seed=None, options=None):\n",
    "###         # We need the following line to seed self.np_random\n",
    "###         super().reset(seed=seed)\n",
    "### \n",
    "###         # Choose the agent's location uniformly at random\n",
    "###         self._agent_location = self.np_random.integers(0, self.size, size=2, dtype=int)\n",
    "### \n",
    "###         # We will sample the target's location randomly until it does not coincide with the agent's location\n",
    "###         self._target_location = self._agent_location\n",
    "###         while np.array_equal(self._target_location, self._agent_location):\n",
    "###             self._target_location = self.np_random.integers(\n",
    "###                 0, self.size, size=2, dtype=int\n",
    "###             )\n",
    "### \n",
    "###         observation = self._get_obs()\n",
    "###         info = self._get_info()\n",
    "### \n",
    "###         if self.render_mode == \"human\":\n",
    "###             self._render_frame()\n",
    "### \n",
    "###         return observation, info\n",
    "###     \n",
    "###     def step(self, action):\n",
    "###         # Map the action (element of {0,1,2,3}) to the direction we walk in\n",
    "###         direction = self._action_to_direction[action]\n",
    "###         # We use `np.clip` to make sure we don't leave the grid\n",
    "###         self._agent_location = np.clip(\n",
    "###             self._agent_location + direction, 0, self.size - 1\n",
    "###         )\n",
    "###         # An episode is done iff the agent has reached the target\n",
    "###         terminated = np.array_equal(self._agent_location, self._target_location)\n",
    "###         reward = 1 if terminated else 0  # Binary sparse rewards\n",
    "###         observation = self._get_obs()\n",
    "###         info = self._get_info()\n",
    "### \n",
    "###         if self.render_mode == \"human\":\n",
    "###             self._render_frame()\n",
    "### \n",
    "###         return observation, reward, terminated, False, info\n",
    "###     \n",
    "###     def render(self):\n",
    "###         if self.render_mode == \"rgb_array\":\n",
    "###             return self._render_frame()\n",
    "### \n",
    "###     def _render_frame(self):\n",
    "###         if self.window is None and self.render_mode == \"human\":\n",
    "###             pygame.init()\n",
    "###             pygame.display.init()\n",
    "###             self.window = pygame.display.set_mode((self.window_size, self.window_size))\n",
    "###         if self.clock is None and self.render_mode == \"human\":\n",
    "###             self.clock = pygame.time.Clock()\n",
    "### \n",
    "###         canvas = pygame.Surface((self.window_size, self.window_size))\n",
    "###         canvas.fill((255, 255, 255))\n",
    "###         pix_square_size = (\n",
    "###             self.window_size / self.size\n",
    "###         )  # The size of a single grid square in pixels\n",
    "### \n",
    "###         # First we draw the target\n",
    "###         pygame.draw.rect(\n",
    "###             canvas,\n",
    "###             (255, 0, 0),\n",
    "###             pygame.Rect(\n",
    "###                 pix_square_size * self._target_location,\n",
    "###                 (pix_square_size, pix_square_size),\n",
    "###             ),\n",
    "###         )\n",
    "###         # Now we draw the agent\n",
    "###         pygame.draw.circle(\n",
    "###             canvas,\n",
    "###             (0, 0, 255),\n",
    "###             (self._agent_location + 0.5) * pix_square_size,\n",
    "###             pix_square_size / 3,\n",
    "###         )\n",
    "### \n",
    "###         # Finally, add some gridlines\n",
    "###         for x in range(self.size + 1):\n",
    "###             pygame.draw.line(\n",
    "###                 canvas,\n",
    "###                 0,\n",
    "###                 (0, pix_square_size * x),\n",
    "###                 (self.window_size, pix_square_size * x),\n",
    "###                 width=3,\n",
    "###             )\n",
    "###             pygame.draw.line(\n",
    "###                 canvas,\n",
    "###                 0,\n",
    "###                 (pix_square_size * x, 0),\n",
    "###                 (pix_square_size * x, self.window_size),\n",
    "###                 width=3,\n",
    "###             )\n",
    "### \n",
    "###         if self.render_mode == \"human\":\n",
    "###             # The following line copies our drawings from `canvas` to the visible window\n",
    "###             self.window.blit(canvas, canvas.get_rect())\n",
    "###             pygame.event.pump()\n",
    "###             pygame.display.update()\n",
    "### \n",
    "###             # We need to ensure that human-rendering occurs at the predefined framerate.\n",
    "###             # The following line will automatically add a delay to keep the framerate stable.\n",
    "###             self.clock.tick(self.metadata[\"render_fps\"])\n",
    "###         else:  # rgb_array\n",
    "###             return np.transpose(\n",
    "###                 np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
    "###             )\n",
    "###         \n",
    "###         def close(self):\n",
    "###             if self.window is not None:\n",
    "###                 pygame.display.quit()\n",
    "###                 pygame.quit()\n",
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5990958c-88eb-4208-a1c4-278d13234f6b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### from gym.envs.registration import register\n",
    "### \n",
    "### register(\n",
    "###     id='gym_examples/TestEnv-v0',\n",
    "###     entry_point='gym_examples.envs:TestEnv',\n",
    "###     max_episode_steps=300,\n",
    "### )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69635b70-e731-49ad-8a90-7d2a89f5d204",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### env = gym.make('gym_examples/TestEnv-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07139d66-407a-4bfb-bf53-8a0c5276b01f",
   "metadata": {},
   "source": [
    "# 2. Gym Example \n",
    "based on https://www.gymlibrary.dev/content/environment_creation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d18c8f0-2dc0-4902-9127-4ac39f801dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import Env\n",
    "from gym import spaces\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import random\n",
    "import pygame\n",
    "import stable_baselines3\n",
    "from stable_baselines3 import PPO, A2C, SAC, TD3, DQN\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e622e9a2-3abb-4a79-9e8b-454d8986f9fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GridWorldEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
    "\n",
    "    def __init__(self, render_mode=None, size=5):\n",
    "        self.size = size  # The size of the square grid\n",
    "        self.window_size = 512  # The size of the PyGame window\n",
    "\n",
    "        # Observations are dictionaries with the agent's and the target's location.\n",
    "        # Each location is encoded as an element of {0, ..., `size`}^2, i.e. MultiDiscrete([size, size]).\n",
    "        self.observation_space = spaces.Dict(\n",
    "            {\n",
    "                \"agent\": spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
    "                \"target\": spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # We have 4 actions, corresponding to \"right\", \"up\", \"left\", \"down\"\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        \"\"\"\n",
    "        The following dictionary maps abstract actions from `self.action_space` to \n",
    "        the direction we will walk in if that action is taken.\n",
    "        I.e. 0 corresponds to \"right\", 1 to \"up\" etc.\n",
    "        \"\"\"\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([1, 0]),\n",
    "            1: np.array([0, 1]),\n",
    "            2: np.array([-1, 0]),\n",
    "            3: np.array([0, -1]),\n",
    "        }\n",
    "\n",
    "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        \"\"\"\n",
    "        If human-rendering is used, `self.window` will be a reference\n",
    "        to the window that we draw to. `self.clock` will be a clock that is used\n",
    "        to ensure that the environment is rendered at the correct framerate in\n",
    "        human-mode. They will remain `None` until human-mode is used for the\n",
    "        first time.\n",
    "        \"\"\"\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "        \n",
    "    def _get_obs(self):\n",
    "        return {\"agent\": self._agent_location, \"target\": self._target_location}\n",
    "\n",
    "    def _get_info(self):\n",
    "        return {\"distance\": np.linalg.norm(self._agent_location - self._target_location, ord=1)}\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # We need the following line to seed self.np_random\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Choose the agent's location uniformly at random\n",
    "        self._agent_location = self.np_random.integers(0, self.size, size=2, dtype=int)\n",
    "\n",
    "        # We will sample the target's location randomly until it does not coincide with the agent's location\n",
    "        self._target_location = self._agent_location\n",
    "        while np.array_equal(self._target_location, self._agent_location):\n",
    "            self._target_location = self.np_random.integers(\n",
    "                0, self.size, size=2, dtype=int\n",
    "            )\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        # Map the action (element of {0,1,2,3}) to the direction we walk in\n",
    "        direction = self._action_to_direction[action]\n",
    "        # We use `np.clip` to make sure we don't leave the grid\n",
    "        self._agent_location = np.clip(\n",
    "            self._agent_location + direction, 0, self.size - 1\n",
    "        )\n",
    "        # An episode is done iff the agent has reached the target\n",
    "        terminated = np.array_equal(self._agent_location, self._target_location)\n",
    "        distance = np.linalg.norm(self._agent_location - self._target_location, ord=1)\n",
    "        reward = 1000 if terminated else - distance  # Binary sparse rewards\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "        # EXPECTED OUTPUTS BY STABLE BASELINE 3\n",
    "        # obs, reward, done, info\n",
    "        # EXPECTED OUTPUT IN STABLE BASELINE3 - PPO.learn\n",
    "        # obs, self.buf_rews[env_idx], terminated, truncated, self.buf_infos[env_idx]\n",
    "        return observation, reward, terminated, False, info\n",
    "    \n",
    "    def render(self):\n",
    "        if self.render_mode == \"rgb_array\":\n",
    "            return self._render_frame()\n",
    "\n",
    "    def _render_frame(self):\n",
    "        if self.window is None and self.render_mode == \"human\":\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.window = pygame.display.set_mode((self.window_size, self.window_size))\n",
    "        if self.clock is None and self.render_mode == \"human\":\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
    "        canvas.fill((255, 255, 255))\n",
    "        pix_square_size = (\n",
    "            self.window_size / self.size\n",
    "        )  # The size of a single grid square in pixels\n",
    "\n",
    "        # First we draw the target\n",
    "        pygame.draw.rect(\n",
    "            canvas,\n",
    "            (255, 0, 0),\n",
    "            pygame.Rect(\n",
    "                pix_square_size * self._target_location,\n",
    "                (pix_square_size, pix_square_size),\n",
    "            ),\n",
    "        )\n",
    "        # Now we draw the agent\n",
    "        pygame.draw.circle(\n",
    "            canvas,\n",
    "            (0, 0, 255),\n",
    "            (self._agent_location + 0.5) * pix_square_size,\n",
    "            pix_square_size / 3,\n",
    "        )\n",
    "\n",
    "        # Finally, add some gridlines\n",
    "        for x in range(self.size + 1):\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (0, pix_square_size * x),\n",
    "                (self.window_size, pix_square_size * x),\n",
    "                width=3,\n",
    "            )\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (pix_square_size * x, 0),\n",
    "                (pix_square_size * x, self.window_size),\n",
    "                width=3,\n",
    "            )\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            # The following line copies our drawings from `canvas` to the visible window\n",
    "            self.window.blit(canvas, canvas.get_rect())\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "\n",
    "            # We need to ensure that human-rendering occurs at the predefined framerate.\n",
    "            # The following line will automatically add a delay to keep the framerate stable.\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "        else:  # rgb_array\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
    "            )\n",
    "\n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c21937c8-d73d-43b1-86f6-5def194bdb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "Genv = GridWorldEnv(size= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "5b06db65-1e99-47b0-99b3-6657bd855632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'agent': array([7, 3]), 'target': array([2, 9])}, {'distance': 11.0})"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Genv.reset(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "abc71664-48e3-42e1-80f9-229e7c3a694a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Genv.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "43995295-2be6-499c-b16b-b499478b8bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('agent', array([5, 2])), ('target', array([9, 8]))])"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Genv.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "d3fb2528-c8a3-43e9-9809-974ebe08d5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 | Steps: 10 | State: [1 0] | Distance: 12.0 | Score: -115.0\n",
      "Episode: 1 | Steps: 10 | State: [2 8] | Distance: 7.0 | Score: -87.0\n",
      "Episode: 2 | Steps: 9 | State: [3 3] | Distance: 0.0 | Score: 984.0\n",
      "Episode: 3 | Steps: 10 | State: [0 2] | Distance: 7.0 | Score: -63.0\n",
      "Episode: 4 | Steps: 10 | State: [7 3] | Distance: 2.0 | Score: -20.0\n",
      "Episode: 5 | Steps: 10 | State: [1 2] | Distance: 3.0 | Score: -30.0\n",
      "Episode: 6 | Steps: 10 | State: [8 6] | Distance: 2.0 | Score: -33.0\n",
      "Episode: 7 | Steps: 10 | State: [3 3] | Distance: 3.0 | Score: -43.0\n",
      "Episode: 8 | Steps: 10 | State: [7 3] | Distance: 7.0 | Score: -37.0\n",
      "Episode: 9 | Steps: 10 | State: [2 9] | Distance: 11.0 | Score: -118.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "for episode in range(episodes):\n",
    "    state = Genv.reset()\n",
    "    terminated = False\n",
    "    score = 0\n",
    "    n_state = 0\n",
    "    \n",
    "    while not terminated:\n",
    "        action = Genv.action_space.sample()\n",
    "        # observation, reward, terminated, False, info\n",
    "        observation, reward, terminated, truncated, distance = Genv.step(action)\n",
    "        score += reward\n",
    "        n_state += 1\n",
    "        if n_state == episodes:\n",
    "            terminated = True\n",
    "    print('Episode: {} | Steps: {} | State: {} | Distance: {} | Score: {}'.format(episode, n_state, observation['agent'], \n",
    "                                                                    distance['distance'], score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "9051810d-f83a-4f78-a1bb-ee42d6b18868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agent': array([4, 6]), 'target': array([5, 1])}"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1819671-cdb0-4e9e-8877-fbe408d0a268",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb53664a-004f-4148-8fa9-24d8ab07a4d7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import copy\n",
    "import warnings\n",
    "from abc import ABC, abstractmethod\n",
    "from functools import partial\n",
    "from typing import Any, Dict, List, Optional, Tuple, Type, TypeVar, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from gym import spaces\n",
    "from torch import nn\n",
    "\n",
    "from stable_baselines3.common.distributions import (\n",
    "    BernoulliDistribution,\n",
    "    CategoricalDistribution,\n",
    "    DiagGaussianDistribution,\n",
    "    Distribution,\n",
    "    MultiCategoricalDistribution,\n",
    "    StateDependentNoiseDistribution,\n",
    "    make_proba_distribution,\n",
    ")\n",
    "from stable_baselines3.common.preprocessing import get_action_dim, is_image_space, maybe_transpose, preprocess_obs\n",
    "from stable_baselines3.common.torch_layers import (\n",
    "    BaseFeaturesExtractor,\n",
    "    CombinedExtractor,\n",
    "    FlattenExtractor,\n",
    "    MlpExtractor,\n",
    "    NatureCNN,\n",
    "    create_mlp,\n",
    ")\n",
    "from stable_baselines3.common.type_aliases import Schedule\n",
    "# from stable_baselines3.common.utils import get_device, is_vectorized_observation, obs_as_tensor\n",
    "\n",
    "SelfBaseModel = TypeVar(\"SelfBaseModel\", bound=\"BaseModel\")\n",
    "\n",
    "\n",
    "class BaseModel(nn.Module):\n",
    "    \"\"\"\n",
    "    The base model object: makes predictions in response to observations.\n",
    "\n",
    "    In the case of policies, the prediction is an action. In the case of critics, it is the\n",
    "    estimated value of the observation.\n",
    "\n",
    "    :param observation_space: The observation space of the environment\n",
    "    :param action_space: The action space of the environment\n",
    "    :param features_extractor_class: Features extractor to use.\n",
    "    :param features_extractor_kwargs: Keyword arguments\n",
    "        to pass to the features extractor.\n",
    "    :param features_extractor: Network to extract features\n",
    "        (a CNN when using images, a nn.Flatten() layer otherwise)\n",
    "    :param normalize_images: Whether to normalize images or not,\n",
    "         dividing by 255.0 (True by default)\n",
    "    :param optimizer_class: The optimizer to use,\n",
    "        ``th.optim.Adam`` by default\n",
    "    :param optimizer_kwargs: Additional keyword arguments,\n",
    "        excluding the learning rate, to pass to the optimizer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_space: spaces.Space,\n",
    "        action_space: spaces.Space,\n",
    "        features_extractor_class: Type[BaseFeaturesExtractor] = FlattenExtractor,\n",
    "        features_extractor_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        features_extractor: Optional[nn.Module] = None,\n",
    "        normalize_images: bool = True,\n",
    "        optimizer_class: Type[th.optim.Optimizer] = th.optim.Adam,\n",
    "        optimizer_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if optimizer_kwargs is None:\n",
    "            optimizer_kwargs = {}\n",
    "\n",
    "        if features_extractor_kwargs is None:\n",
    "            features_extractor_kwargs = {}\n",
    "\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "        self.features_extractor = features_extractor\n",
    "        self.normalize_images = normalize_images\n",
    "\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.optimizer_kwargs = optimizer_kwargs\n",
    "        self.optimizer = None  # type: Optional[th.optim.Optimizer]\n",
    "\n",
    "        self.features_extractor_class = features_extractor_class\n",
    "        self.features_extractor_kwargs = features_extractor_kwargs\n",
    "        # Automatically deactivate dtype and bounds checks\n",
    "        if normalize_images is False and issubclass(features_extractor_class, (NatureCNN, CombinedExtractor)):\n",
    "            self.features_extractor_kwargs.update(dict(normalized_image=True))\n",
    "\n",
    "    def _update_features_extractor(\n",
    "        self,\n",
    "        net_kwargs: Dict[str, Any],\n",
    "        features_extractor: Optional[BaseFeaturesExtractor] = None,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Update the network keyword arguments and create a new features extractor object if needed.\n",
    "        If a ``features_extractor`` object is passed, then it will be shared.\n",
    "\n",
    "        :param net_kwargs: the base network keyword arguments, without the ones\n",
    "            related to features extractor\n",
    "        :param features_extractor: a features extractor object.\n",
    "            If None, a new object will be created.\n",
    "        :return: The updated keyword arguments\n",
    "        \"\"\"\n",
    "        net_kwargs = net_kwargs.copy()\n",
    "        if features_extractor is None:\n",
    "            # The features extractor is not shared, create a new one\n",
    "            features_extractor = self.make_features_extractor()\n",
    "        net_kwargs.update(dict(features_extractor=features_extractor, features_dim=features_extractor.features_dim))\n",
    "        return net_kwargs\n",
    "\n",
    "    def make_features_extractor(self) -> BaseFeaturesExtractor:\n",
    "        \"\"\"Helper method to create a features extractor.\"\"\"\n",
    "        return self.features_extractor_class(self.observation_space, **self.features_extractor_kwargs)\n",
    "\n",
    "    def extract_features(self, obs: th.Tensor, features_extractor: BaseFeaturesExtractor) -> th.Tensor:\n",
    "        \"\"\"\n",
    "        Preprocess the observation if needed and extract features.\n",
    "\n",
    "         :param obs: The observation\n",
    "         :param features_extractor: The features extractor to use.\n",
    "         :return: The extracted features\n",
    "        \"\"\"\n",
    "        preprocessed_obs = preprocess_obs(obs, self.observation_space, normalize_images=self.normalize_images)\n",
    "        return features_extractor(preprocessed_obs)\n",
    "\n",
    "    def _get_constructor_parameters(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get data that need to be saved in order to re-create the model when loading it from disk.\n",
    "\n",
    "        :return: The dictionary to pass to the as kwargs constructor when reconstruction this model.\n",
    "        \"\"\"\n",
    "        return dict(\n",
    "            observation_space=self.observation_space,\n",
    "            action_space=self.action_space,\n",
    "            # Passed to the constructor by child class\n",
    "            # squash_output=self.squash_output,\n",
    "            # features_extractor=self.features_extractor\n",
    "            normalize_images=self.normalize_images,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def device(self) -> th.device:\n",
    "        \"\"\"Infer which device this policy lives on by inspecting its parameters.\n",
    "        If it has no parameters, the 'cpu' device is used as a fallback.\n",
    "\n",
    "        :return:\"\"\"\n",
    "        for param in self.parameters():\n",
    "            return param.device\n",
    "        return get_device(\"cpu\")\n",
    "\n",
    "    def save(self, path: str) -> None:\n",
    "        \"\"\"\n",
    "        Save model to a given location.\n",
    "\n",
    "        :param path:\n",
    "        \"\"\"\n",
    "        th.save({\"state_dict\": self.state_dict(), \"data\": self._get_constructor_parameters()}, path)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls: Type[SelfBaseModel], path: str, device: Union[th.device, str] = \"auto\") -> SelfBaseModel:\n",
    "        \"\"\"\n",
    "        Load model from path.\n",
    "\n",
    "        :param path:\n",
    "        :param device: Device on which the policy should be loaded.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        device = get_device(device)\n",
    "        saved_variables = th.load(path, map_location=device)\n",
    "\n",
    "        # Create policy object\n",
    "        model = cls(**saved_variables[\"data\"])  # pytype: disable=not-instantiable\n",
    "        # Load weights\n",
    "        model.load_state_dict(saved_variables[\"state_dict\"])\n",
    "        model.to(device)\n",
    "        return model\n",
    "\n",
    "    def load_from_vector(self, vector: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Load parameters from a 1D vector.\n",
    "\n",
    "        :param vector:\n",
    "        \"\"\"\n",
    "        th.nn.utils.vector_to_parameters(th.as_tensor(vector, dtype=th.float, device=self.device), self.parameters())\n",
    "\n",
    "    def parameters_to_vector(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Convert the parameters to a 1D vector.\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return th.nn.utils.parameters_to_vector(self.parameters()).detach().cpu().numpy()\n",
    "\n",
    "    def set_training_mode(self, mode: bool) -> None:\n",
    "        \"\"\"\n",
    "        Put the policy in either training or evaluation mode.\n",
    "\n",
    "        This affects certain modules, such as batch normalisation and dropout.\n",
    "\n",
    "        :param mode: if true, set to training mode, else set to evaluation mode\n",
    "        \"\"\"\n",
    "        self.train(mode)\n",
    "\n",
    "    def obs_to_tensor(self, observation: Union[np.ndarray, Dict[str, np.ndarray]]) -> Tuple[th.Tensor, bool]:\n",
    "        \"\"\"\n",
    "        Convert an input observation to a PyTorch tensor that can be fed to a model.\n",
    "        Includes sugar-coating to handle different observations (e.g. normalizing images).\n",
    "\n",
    "        :param observation: the input observation\n",
    "        :return: The observation as PyTorch tensor\n",
    "            and whether the observation is vectorized or not\n",
    "        \"\"\"\n",
    "        vectorized_env = False\n",
    "        if isinstance(observation, dict):\n",
    "            # need to copy the dict as the dict in VecFrameStack will become a torch tensor\n",
    "            observation = copy.deepcopy(observation)\n",
    "            for key, obs in observation.items():\n",
    "                obs_space = self.observation_space.spaces[key]\n",
    "                if is_image_space(obs_space):\n",
    "                    obs_ = maybe_transpose(obs, obs_space)\n",
    "                else:\n",
    "                    obs_ = np.array(obs)\n",
    "                vectorized_env = vectorized_env or is_vectorized_observation(obs_, obs_space)\n",
    "                # Add batch dimension if needed\n",
    "                observation[key] = obs_.reshape((-1,) + self.observation_space[key].shape)\n",
    "\n",
    "        elif is_image_space(self.observation_space):\n",
    "            # Handle the different cases for images\n",
    "            # as PyTorch use channel first format\n",
    "            observation = maybe_transpose(observation, self.observation_space)\n",
    "\n",
    "        else:\n",
    "            observation = np.array(observation)\n",
    "\n",
    "        if not isinstance(observation, dict):\n",
    "            # Dict obs need to be handled separately\n",
    "            vectorized_env = is_vectorized_observation(observation, self.observation_space)\n",
    "            # Add batch dimension if needed\n",
    "            observation = observation.reshape((-1,) + self.observation_space.shape)\n",
    "\n",
    "        observation = obs_as_tensor(observation, self.device)\n",
    "        return observation, vectorized_env\n",
    "\n",
    "\n",
    "class BasePolicy(BaseModel, ABC):\n",
    "    \"\"\"The base policy object.\n",
    "\n",
    "    Parameters are mostly the same as `BaseModel`; additions are documented below.\n",
    "\n",
    "    :param args: positional arguments passed through to `BaseModel`.\n",
    "    :param kwargs: keyword arguments passed through to `BaseModel`.\n",
    "    :param squash_output: For continuous actions, whether the output is squashed\n",
    "        or not using a ``tanh()`` function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, squash_output: bool = False, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._squash_output = squash_output\n",
    "\n",
    "    @staticmethod\n",
    "    def _dummy_schedule(progress_remaining: float) -> float:\n",
    "        \"\"\"(float) Useful for pickling policy.\"\"\"\n",
    "        del progress_remaining\n",
    "        return 0.0\n",
    "\n",
    "    @property\n",
    "    def squash_output(self) -> bool:\n",
    "        \"\"\"(bool) Getter for squash_output.\"\"\"\n",
    "        return self._squash_output\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weights(module: nn.Module, gain: float = 1) -> None:\n",
    "        \"\"\"\n",
    "        Orthogonal initialization (used in PPO and A2C)\n",
    "        \"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "            nn.init.orthogonal_(module.weight, gain=gain)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.fill_(0.0)\n",
    "\n",
    "    @abstractmethod\n",
    "    def _predict(self, observation: th.Tensor, deterministic: bool = False) -> th.Tensor:\n",
    "        \"\"\"\n",
    "        Get the action according to the policy for a given observation.\n",
    "\n",
    "        By default provides a dummy implementation -- not all BasePolicy classes\n",
    "        implement this, e.g. if they are a Critic in an Actor-Critic method.\n",
    "\n",
    "        :param observation:\n",
    "        :param deterministic: Whether to use stochastic or deterministic actions\n",
    "        :return: Taken action according to the policy\n",
    "        \"\"\"\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        observation: Union[np.ndarray, Dict[str, np.ndarray]],\n",
    "        state: Optional[Tuple[np.ndarray, ...]] = None,\n",
    "        episode_start: Optional[np.ndarray] = None,\n",
    "        deterministic: bool = False,\n",
    "    ) -> Tuple[np.ndarray, Optional[Tuple[np.ndarray, ...]]]:\n",
    "        \"\"\"\n",
    "        Get the policy action from an observation (and optional hidden state).\n",
    "        Includes sugar-coating to handle different observations (e.g. normalizing images).\n",
    "\n",
    "        :param observation: the input observation\n",
    "        :param state: The last hidden states (can be None, used in recurrent policies)\n",
    "        :param episode_start: The last masks (can be None, used in recurrent policies)\n",
    "            this correspond to beginning of episodes,\n",
    "            where the hidden states of the RNN must be reset.\n",
    "        :param deterministic: Whether or not to return deterministic actions.\n",
    "        :return: the model's action and the next hidden state\n",
    "            (used in recurrent policies)\n",
    "        \"\"\"\n",
    "        # TODO (GH/1): add support for RNN policies\n",
    "        # if state is None:\n",
    "        #     state = self.initial_state\n",
    "        # if episode_start is None:\n",
    "        #     episode_start = [False for _ in range(self.n_envs)]\n",
    "        # Switch to eval mode (this affects batch norm / dropout)\n",
    "        self.set_training_mode(False)\n",
    "\n",
    "        observation, vectorized_env = self.obs_to_tensor(observation)\n",
    "\n",
    "        with th.no_grad():\n",
    "            actions = self._predict(observation, deterministic=deterministic)\n",
    "        # Convert to numpy, and reshape to the original action shape\n",
    "        actions = actions.cpu().numpy().reshape((-1,) + self.action_space.shape)\n",
    "\n",
    "        if isinstance(self.action_space, spaces.Box):\n",
    "            if self.squash_output:\n",
    "                # Rescale to proper domain when using squashing\n",
    "                actions = self.unscale_action(actions)\n",
    "            else:\n",
    "                # Actions could be on arbitrary scale, so clip the actions to avoid\n",
    "                # out of bound error (e.g. if sampling from a Gaussian distribution)\n",
    "                actions = np.clip(actions, self.action_space.low, self.action_space.high)\n",
    "\n",
    "        # Remove batch dimension if needed\n",
    "        if not vectorized_env:\n",
    "            actions = actions.squeeze(axis=0)\n",
    "\n",
    "        return actions, state\n",
    "\n",
    "    def scale_action(self, action: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Rescale the action from [low, high] to [-1, 1]\n",
    "        (no need for symmetric action space)\n",
    "\n",
    "        :param action: Action to scale\n",
    "        :return: Scaled action\n",
    "        \"\"\"\n",
    "        low, high = self.action_space.low, self.action_space.high\n",
    "        return 2.0 * ((action - low) / (high - low)) - 1.0\n",
    "\n",
    "    def unscale_action(self, scaled_action: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Rescale the action from [-1, 1] to [low, high]\n",
    "        (no need for symmetric action space)\n",
    "\n",
    "        :param scaled_action: Action to un-scale\n",
    "        \"\"\"\n",
    "        low, high = self.action_space.low, self.action_space.high\n",
    "        return low + (0.5 * (scaled_action + 1.0) * (high - low))\n",
    "\n",
    "\n",
    "class ActorCriticPolicy(BasePolicy):\n",
    "    \"\"\"\n",
    "    Policy class for actor-critic algorithms (has both policy and value prediction).\n",
    "    Used by A2C, PPO and the likes.\n",
    "\n",
    "    :param observation_space: Observation space\n",
    "    :param action_space: Action space\n",
    "    :param lr_schedule: Learning rate schedule (could be constant)\n",
    "    :param net_arch: The specification of the policy and value networks.\n",
    "    :param activation_fn: Activation function\n",
    "    :param ortho_init: Whether to use or not orthogonal initialization\n",
    "    :param use_sde: Whether to use State Dependent Exploration or not\n",
    "    :param log_std_init: Initial value for the log standard deviation\n",
    "    :param full_std: Whether to use (n_features x n_actions) parameters\n",
    "        for the std instead of only (n_features,) when using gSDE\n",
    "    :param use_expln: Use ``expln()`` function instead of ``exp()`` to ensure\n",
    "        a positive standard deviation (cf paper). It allows to keep variance\n",
    "        above zero and prevent it from growing too fast. In practice, ``exp()`` is usually enough.\n",
    "    :param squash_output: Whether to squash the output using a tanh function,\n",
    "        this allows to ensure boundaries when using gSDE.\n",
    "    :param features_extractor_class: Features extractor to use.\n",
    "    :param features_extractor_kwargs: Keyword arguments\n",
    "        to pass to the features extractor.\n",
    "    :param share_features_extractor: If True, the features extractor is shared between the policy and value networks.\n",
    "    :param normalize_images: Whether to normalize images or not,\n",
    "         dividing by 255.0 (True by default)\n",
    "    :param optimizer_class: The optimizer to use,\n",
    "        ``th.optim.Adam`` by default\n",
    "    :param optimizer_kwargs: Additional keyword arguments,\n",
    "        excluding the learning rate, to pass to the optimizer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_space: spaces.Space,\n",
    "        action_space: spaces.Space,\n",
    "        lr_schedule: Schedule,\n",
    "        net_arch: Optional[Union[List[int], Dict[str, List[int]]]] = None,\n",
    "        activation_fn: Type[nn.Module] = nn.Tanh,\n",
    "        ortho_init: bool = True,\n",
    "        use_sde: bool = False,\n",
    "        log_std_init: float = 0.0,\n",
    "        full_std: bool = True,\n",
    "        use_expln: bool = False,\n",
    "        squash_output: bool = False,\n",
    "        features_extractor_class: Type[BaseFeaturesExtractor] = FlattenExtractor,\n",
    "        features_extractor_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        share_features_extractor: bool = True,\n",
    "        normalize_images: bool = True,\n",
    "        optimizer_class: Type[th.optim.Optimizer] = th.optim.Adam,\n",
    "        optimizer_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    ):\n",
    "        if optimizer_kwargs is None:\n",
    "            optimizer_kwargs = {}\n",
    "            # Small values to avoid NaN in Adam optimizer\n",
    "            if optimizer_class == th.optim.Adam:\n",
    "                optimizer_kwargs[\"eps\"] = 1e-5\n",
    "\n",
    "        super().__init__(\n",
    "            observation_space,\n",
    "            action_space,\n",
    "            features_extractor_class,\n",
    "            features_extractor_kwargs,\n",
    "            optimizer_class=optimizer_class,\n",
    "            optimizer_kwargs=optimizer_kwargs,\n",
    "            squash_output=squash_output,\n",
    "            normalize_images=normalize_images,\n",
    "        )\n",
    "\n",
    "        if isinstance(net_arch, list) and len(net_arch) > 0 and isinstance(net_arch[0], dict):\n",
    "            warnings.warn(\n",
    "                (\n",
    "                    \"As shared layers in the mlp_extractor are removed since SB3 v1.8.0, \"\n",
    "                    \"you should now pass directly a dictionary and not a list \"\n",
    "                    \"(net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])\"\n",
    "                ),\n",
    "            )\n",
    "            net_arch = net_arch[0]\n",
    "\n",
    "        # Default network architecture, from stable-baselines\n",
    "        if net_arch is None:\n",
    "            if features_extractor_class == NatureCNN:\n",
    "                net_arch = []\n",
    "            else:\n",
    "                net_arch = dict(pi=[64, 64], vf=[64, 64])\n",
    "\n",
    "        self.net_arch = net_arch\n",
    "        self.activation_fn = activation_fn\n",
    "        self.ortho_init = ortho_init\n",
    "\n",
    "        self.share_features_extractor = share_features_extractor\n",
    "        self.features_extractor = self.make_features_extractor()\n",
    "        self.features_dim = self.features_extractor.features_dim\n",
    "        if self.share_features_extractor:\n",
    "            self.pi_features_extractor = self.features_extractor\n",
    "            self.vf_features_extractor = self.features_extractor\n",
    "        else:\n",
    "            self.pi_features_extractor = self.features_extractor\n",
    "            self.vf_features_extractor = self.make_features_extractor()\n",
    "\n",
    "        self.log_std_init = log_std_init\n",
    "        dist_kwargs = None\n",
    "        # Keyword arguments for gSDE distribution\n",
    "        if use_sde:\n",
    "            dist_kwargs = {\n",
    "                \"full_std\": full_std,\n",
    "                \"squash_output\": squash_output,\n",
    "                \"use_expln\": use_expln,\n",
    "                \"learn_features\": False,\n",
    "            }\n",
    "\n",
    "        self.use_sde = use_sde\n",
    "        self.dist_kwargs = dist_kwargs\n",
    "\n",
    "        # Action distribution\n",
    "        self.action_dist = make_proba_distribution(action_space, use_sde=use_sde, dist_kwargs=dist_kwargs)\n",
    "\n",
    "        self._build(lr_schedule)\n",
    "\n",
    "    def _get_constructor_parameters(self) -> Dict[str, Any]:\n",
    "        data = super()._get_constructor_parameters()\n",
    "\n",
    "        default_none_kwargs = self.dist_kwargs or collections.defaultdict(lambda: None)\n",
    "\n",
    "        data.update(\n",
    "            dict(\n",
    "                net_arch=self.net_arch,\n",
    "                activation_fn=self.activation_fn,\n",
    "                use_sde=self.use_sde,\n",
    "                log_std_init=self.log_std_init,\n",
    "                squash_output=default_none_kwargs[\"squash_output\"],\n",
    "                full_std=default_none_kwargs[\"full_std\"],\n",
    "                use_expln=default_none_kwargs[\"use_expln\"],\n",
    "                lr_schedule=self._dummy_schedule,  # dummy lr schedule, not needed for loading policy alone\n",
    "                ortho_init=self.ortho_init,\n",
    "                optimizer_class=self.optimizer_class,\n",
    "                optimizer_kwargs=self.optimizer_kwargs,\n",
    "                features_extractor_class=self.features_extractor_class,\n",
    "                features_extractor_kwargs=self.features_extractor_kwargs,\n",
    "            )\n",
    "        )\n",
    "        return data\n",
    "\n",
    "    def reset_noise(self, n_envs: int = 1) -> None:\n",
    "        \"\"\"\n",
    "        Sample new weights for the exploration matrix.\n",
    "\n",
    "        :param n_envs:\n",
    "        \"\"\"\n",
    "        assert isinstance(self.action_dist, StateDependentNoiseDistribution), \"reset_noise() is only available when using gSDE\"\n",
    "        self.action_dist.sample_weights(self.log_std, batch_size=n_envs)\n",
    "\n",
    "\n",
    "    def _build_mlp_extractor(self) -> None:\n",
    "        \"\"\"\n",
    "        Create the policy and value networks.\n",
    "        Part of the layers can be shared.\n",
    "        \"\"\"\n",
    "        # Note: If net_arch is None and some features extractor is used,\n",
    "        #       net_arch here is an empty list and mlp_extractor does not\n",
    "        #       really contain any layers (acts like an identity module).\n",
    "        self.mlp_extractor = MlpExtractor(\n",
    "            self.features_dim,\n",
    "            net_arch=self.net_arch,\n",
    "            activation_fn=self.activation_fn,\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "    def _build(self, lr_schedule: Schedule) -> None:\n",
    "        \"\"\"\n",
    "        Create the networks and the optimizer.\n",
    "\n",
    "        :param lr_schedule: Learning rate schedule\n",
    "            lr_schedule(1) is the initial learning rate\n",
    "        \"\"\"\n",
    "        self._build_mlp_extractor()\n",
    "\n",
    "        latent_dim_pi = self.mlp_extractor.latent_dim_pi\n",
    "\n",
    "        if isinstance(self.action_dist, DiagGaussianDistribution):\n",
    "            self.action_net, self.log_std = self.action_dist.proba_distribution_net(\n",
    "                latent_dim=latent_dim_pi, log_std_init=self.log_std_init\n",
    "            )\n",
    "        elif isinstance(self.action_dist, StateDependentNoiseDistribution):\n",
    "            self.action_net, self.log_std = self.action_dist.proba_distribution_net(\n",
    "                latent_dim=latent_dim_pi, latent_sde_dim=latent_dim_pi, log_std_init=self.log_std_init\n",
    "            )\n",
    "        elif isinstance(self.action_dist, (CategoricalDistribution, MultiCategoricalDistribution, BernoulliDistribution)):\n",
    "            self.action_net = self.action_dist.proba_distribution_net(latent_dim=latent_dim_pi)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Unsupported distribution '{self.action_dist}'.\")\n",
    "\n",
    "        self.value_net = nn.Linear(self.mlp_extractor.latent_dim_vf, 1)\n",
    "        # Init weights: use orthogonal initialization\n",
    "        # with small initial weight for the output\n",
    "        if self.ortho_init:\n",
    "            # TODO: check for features_extractor\n",
    "            # Values from stable-baselines.\n",
    "            # features_extractor/mlp values are\n",
    "            # originally from openai/baselines (default gains/init_scales).\n",
    "            module_gains = {\n",
    "                self.features_extractor: np.sqrt(2),\n",
    "                self.mlp_extractor: np.sqrt(2),\n",
    "                self.action_net: 0.01,\n",
    "                self.value_net: 1,\n",
    "            }\n",
    "            if not self.share_features_extractor:\n",
    "                # Note(antonin): this is to keep SB3 results\n",
    "                # consistent, see GH#1148\n",
    "                del module_gains[self.features_extractor]\n",
    "                module_gains[self.pi_features_extractor] = np.sqrt(2)\n",
    "                module_gains[self.vf_features_extractor] = np.sqrt(2)\n",
    "\n",
    "            for module, gain in module_gains.items():\n",
    "                module.apply(partial(self.init_weights, gain=gain))\n",
    "\n",
    "        # Setup optimizer with initial learning rate\n",
    "        self.optimizer = self.optimizer_class(self.parameters(), lr=lr_schedule(1), **self.optimizer_kwargs)\n",
    "\n",
    "    def forward(self, obs: th.Tensor, deterministic: bool = False) -> Tuple[th.Tensor, th.Tensor, th.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass in all the networks (actor and critic)\n",
    "\n",
    "        :param obs: Observation\n",
    "        :param deterministic: Whether to sample or use deterministic actions\n",
    "        :return: action, value and log probability of the action\n",
    "        \"\"\"\n",
    "        # Preprocess the observation if needed\n",
    "        features = self.extract_features(obs)\n",
    "        if self.share_features_extractor:\n",
    "            latent_pi, latent_vf = self.mlp_extractor(features)\n",
    "        else:\n",
    "            pi_features, vf_features = features\n",
    "            latent_pi = self.mlp_extractor.forward_actor(pi_features)\n",
    "            latent_vf = self.mlp_extractor.forward_critic(vf_features)\n",
    "        # Evaluate the values for the given observations\n",
    "        values = self.value_net(latent_vf)\n",
    "        distribution = self._get_action_dist_from_latent(latent_pi)\n",
    "        actions = distribution.get_actions(deterministic=deterministic)\n",
    "        log_prob = distribution.log_prob(actions)\n",
    "        actions = actions.reshape((-1,) + self.action_space.shape)\n",
    "        return actions, values, log_prob\n",
    "\n",
    "\n",
    "    def extract_features(self, obs: th.Tensor) -> Union[th.Tensor, Tuple[th.Tensor, th.Tensor]]:\n",
    "        \"\"\"\n",
    "        Preprocess the observation if needed and extract features.\n",
    "\n",
    "        :param obs: Observation\n",
    "        :return: the output of the features extractor(s)\n",
    "        \"\"\"\n",
    "        if self.share_features_extractor:\n",
    "            return super().extract_features(obs, self.features_extractor)\n",
    "        else:\n",
    "            pi_features = super().extract_features(obs, self.pi_features_extractor)\n",
    "            vf_features = super().extract_features(obs, self.vf_features_extractor)\n",
    "            return pi_features, vf_features\n",
    "\n",
    "\n",
    "    def _get_action_dist_from_latent(self, latent_pi: th.Tensor) -> Distribution:\n",
    "        \"\"\"\n",
    "        Retrieve action distribution given the latent codes.\n",
    "\n",
    "        :param latent_pi: Latent code for the actor\n",
    "        :return: Action distribution\n",
    "        \"\"\"\n",
    "        mean_actions = self.action_net(latent_pi)\n",
    "\n",
    "        if isinstance(self.action_dist, DiagGaussianDistribution):\n",
    "            return self.action_dist.proba_distribution(mean_actions, self.log_std)\n",
    "        elif isinstance(self.action_dist, CategoricalDistribution):\n",
    "            # Here mean_actions are the logits before the softmax\n",
    "            return self.action_dist.proba_distribution(action_logits=mean_actions)\n",
    "        elif isinstance(self.action_dist, MultiCategoricalDistribution):\n",
    "            # Here mean_actions are the flattened logits\n",
    "            return self.action_dist.proba_distribution(action_logits=mean_actions)\n",
    "        elif isinstance(self.action_dist, BernoulliDistribution):\n",
    "            # Here mean_actions are the logits (before rounding to get the binary actions)\n",
    "            return self.action_dist.proba_distribution(action_logits=mean_actions)\n",
    "        elif isinstance(self.action_dist, StateDependentNoiseDistribution):\n",
    "            return self.action_dist.proba_distribution(mean_actions, self.log_std, latent_pi)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action distribution\")\n",
    "\n",
    "    def _predict(self, observation: th.Tensor, deterministic: bool = False) -> th.Tensor:\n",
    "        \"\"\"\n",
    "        Get the action according to the policy for a given observation.\n",
    "\n",
    "        :param observation:\n",
    "        :param deterministic: Whether to use stochastic or deterministic actions\n",
    "        :return: Taken action according to the policy\n",
    "        \"\"\"\n",
    "        return self.get_distribution(observation).get_actions(deterministic=deterministic)\n",
    "\n",
    "    def evaluate_actions(self, obs: th.Tensor, actions: th.Tensor) -> Tuple[th.Tensor, th.Tensor, Optional[th.Tensor]]:\n",
    "        \"\"\"\n",
    "        Evaluate actions according to the current policy,\n",
    "        given the observations.\n",
    "\n",
    "        :param obs: Observation\n",
    "        :param actions: Actions\n",
    "        :return: estimated value, log likelihood of taking those actions\n",
    "            and entropy of the action distribution.\n",
    "        \"\"\"\n",
    "        # Preprocess the observation if needed\n",
    "        features = self.extract_features(obs)\n",
    "        if self.share_features_extractor:\n",
    "            latent_pi, latent_vf = self.mlp_extractor(features)\n",
    "        else:\n",
    "            pi_features, vf_features = features\n",
    "            latent_pi = self.mlp_extractor.forward_actor(pi_features)\n",
    "            latent_vf = self.mlp_extractor.forward_critic(vf_features)\n",
    "        distribution = self._get_action_dist_from_latent(latent_pi)\n",
    "        log_prob = distribution.log_prob(actions)\n",
    "        values = self.value_net(latent_vf)\n",
    "        entropy = distribution.entropy()\n",
    "        return values, log_prob, entropy\n",
    "\n",
    "\n",
    "    def get_distribution(self, obs: th.Tensor) -> Distribution:\n",
    "        \"\"\"\n",
    "        Get the current policy distribution given the observations.\n",
    "\n",
    "        :param obs:\n",
    "        :return: the action distribution.\n",
    "        \"\"\"\n",
    "        features = super().extract_features(obs, self.pi_features_extractor)\n",
    "        latent_pi = self.mlp_extractor.forward_actor(features)\n",
    "        return self._get_action_dist_from_latent(latent_pi)\n",
    "\n",
    "\n",
    "    def predict_values(self, obs: th.Tensor) -> th.Tensor:\n",
    "        \"\"\"\n",
    "        Get the estimated values according to the current policy given the observations.\n",
    "\n",
    "        :param obs: Observation\n",
    "        :return: the estimated values.\n",
    "        \"\"\"\n",
    "        features = super().extract_features(obs, self.vf_features_extractor)\n",
    "        latent_vf = self.mlp_extractor.forward_critic(features)\n",
    "        return self.value_net(latent_vf)\n",
    "\n",
    "\n",
    "\n",
    "class ActorCriticCnnPolicy(ActorCriticPolicy):\n",
    "    \"\"\"\n",
    "    CNN policy class for actor-critic algorithms (has both policy and value prediction).\n",
    "    Used by A2C, PPO and the likes.\n",
    "\n",
    "    :param observation_space: Observation space\n",
    "    :param action_space: Action space\n",
    "    :param lr_schedule: Learning rate schedule (could be constant)\n",
    "    :param net_arch: The specification of the policy and value networks.\n",
    "    :param activation_fn: Activation function\n",
    "    :param ortho_init: Whether to use or not orthogonal initialization\n",
    "    :param use_sde: Whether to use State Dependent Exploration or not\n",
    "    :param log_std_init: Initial value for the log standard deviation\n",
    "    :param full_std: Whether to use (n_features x n_actions) parameters\n",
    "        for the std instead of only (n_features,) when using gSDE\n",
    "    :param use_expln: Use ``expln()`` function instead of ``exp()`` to ensure\n",
    "        a positive standard deviation (cf paper). It allows to keep variance\n",
    "        above zero and prevent it from growing too fast. In practice, ``exp()`` is usually enough.\n",
    "    :param squash_output: Whether to squash the output using a tanh function,\n",
    "        this allows to ensure boundaries when using gSDE.\n",
    "    :param features_extractor_class: Features extractor to use.\n",
    "    :param features_extractor_kwargs: Keyword arguments\n",
    "        to pass to the features extractor.\n",
    "    :param share_features_extractor: If True, the features extractor is shared between the policy and value networks.\n",
    "    :param normalize_images: Whether to normalize images or not,\n",
    "         dividing by 255.0 (True by default)\n",
    "    :param optimizer_class: The optimizer to use,\n",
    "        ``th.optim.Adam`` by default\n",
    "    :param optimizer_kwargs: Additional keyword arguments,\n",
    "        excluding the learning rate, to pass to the optimizer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_space: spaces.Space,\n",
    "        action_space: spaces.Space,\n",
    "        lr_schedule: Schedule,\n",
    "        net_arch: Optional[Union[List[int], Dict[str, List[int]]]] = None,\n",
    "        activation_fn: Type[nn.Module] = nn.Tanh,\n",
    "        ortho_init: bool = True,\n",
    "        use_sde: bool = False,\n",
    "        log_std_init: float = 0.0,\n",
    "        full_std: bool = True,\n",
    "        use_expln: bool = False,\n",
    "        squash_output: bool = False,\n",
    "        features_extractor_class: Type[BaseFeaturesExtractor] = NatureCNN,\n",
    "        features_extractor_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        share_features_extractor: bool = True,\n",
    "        normalize_images: bool = True,\n",
    "        optimizer_class: Type[th.optim.Optimizer] = th.optim.Adam,\n",
    "        optimizer_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            observation_space,\n",
    "            action_space,\n",
    "            lr_schedule,\n",
    "            net_arch,\n",
    "            activation_fn,\n",
    "            ortho_init,\n",
    "            use_sde,\n",
    "            log_std_init,\n",
    "            full_std,\n",
    "            use_expln,\n",
    "            squash_output,\n",
    "            features_extractor_class,\n",
    "            features_extractor_kwargs,\n",
    "            share_features_extractor,\n",
    "            normalize_images,\n",
    "            optimizer_class,\n",
    "            optimizer_kwargs,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "class MultiInputActorCriticPolicy(ActorCriticPolicy):\n",
    "    \"\"\"\n",
    "    MultiInputActorClass policy class for actor-critic algorithms (has both policy and value prediction).\n",
    "    Used by A2C, PPO and the likes.\n",
    "\n",
    "    :param observation_space: Observation space (Tuple)\n",
    "    :param action_space: Action space\n",
    "    :param lr_schedule: Learning rate schedule (could be constant)\n",
    "    :param net_arch: The specification of the policy and value networks.\n",
    "    :param activation_fn: Activation function\n",
    "    :param ortho_init: Whether to use or not orthogonal initialization\n",
    "    :param use_sde: Whether to use State Dependent Exploration or not\n",
    "    :param log_std_init: Initial value for the log standard deviation\n",
    "    :param full_std: Whether to use (n_features x n_actions) parameters\n",
    "        for the std instead of only (n_features,) when using gSDE\n",
    "    :param use_expln: Use ``expln()`` function instead of ``exp()`` to ensure\n",
    "        a positive standard deviation (cf paper). It allows to keep variance\n",
    "        above zero and prevent it from growing too fast. In practice, ``exp()`` is usually enough.\n",
    "    :param squash_output: Whether to squash the output using a tanh function,\n",
    "        this allows to ensure boundaries when using gSDE.\n",
    "    :param features_extractor_class: Uses the CombinedExtractor\n",
    "    :param features_extractor_kwargs: Keyword arguments\n",
    "        to pass to the features extractor.\n",
    "    :param share_features_extractor: If True, the features extractor is shared between the policy and value networks.\n",
    "    :param normalize_images: Whether to normalize images or not,\n",
    "         dividing by 255.0 (True by default)\n",
    "    :param optimizer_class: The optimizer to use,\n",
    "        ``th.optim.Adam`` by default\n",
    "    :param optimizer_kwargs: Additional keyword arguments,\n",
    "        excluding the learning rate, to pass to the optimizer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_space: spaces.Dict,\n",
    "        action_space: spaces.Space,\n",
    "        lr_schedule: Schedule,\n",
    "        net_arch: Optional[Union[List[int], Dict[str, List[int]]]] = None,\n",
    "        activation_fn: Type[nn.Module] = nn.Tanh,\n",
    "        ortho_init: bool = True,\n",
    "        use_sde: bool = False,\n",
    "        log_std_init: float = 0.0,\n",
    "        full_std: bool = True,\n",
    "        use_expln: bool = False,\n",
    "        squash_output: bool = False,\n",
    "        features_extractor_class: Type[BaseFeaturesExtractor] = CombinedExtractor,\n",
    "        features_extractor_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        share_features_extractor: bool = True,\n",
    "        normalize_images: bool = True,\n",
    "        optimizer_class: Type[th.optim.Optimizer] = th.optim.Adam,\n",
    "        optimizer_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            observation_space,\n",
    "            action_space,\n",
    "            lr_schedule,\n",
    "            net_arch,\n",
    "            activation_fn,\n",
    "            ortho_init,\n",
    "            use_sde,\n",
    "            log_std_init,\n",
    "            full_std,\n",
    "            use_expln,\n",
    "            squash_output,\n",
    "            features_extractor_class,\n",
    "            features_extractor_kwargs,\n",
    "            share_features_extractor,\n",
    "            normalize_images,\n",
    "            optimizer_class,\n",
    "            optimizer_kwargs,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "class ContinuousCritic(BaseModel):\n",
    "    \"\"\"\n",
    "    Critic network(s) for DDPG/SAC/TD3.\n",
    "    It represents the action-state value function (Q-value function).\n",
    "    Compared to A2C/PPO critics, this one represents the Q-value\n",
    "    and takes the continuous action as input. It is concatenated with the state\n",
    "    and then fed to the network which outputs a single value: Q(s, a).\n",
    "    For more recent algorithms like SAC/TD3, multiple networks\n",
    "    are created to give different estimates.\n",
    "\n",
    "    By default, it creates two critic networks used to reduce overestimation\n",
    "    thanks to clipped Q-learning (cf TD3 paper).\n",
    "\n",
    "    :param observation_space: Obervation space\n",
    "    :param action_space: Action space\n",
    "    :param net_arch: Network architecture\n",
    "    :param features_extractor: Network to extract features\n",
    "        (a CNN when using images, a nn.Flatten() layer otherwise)\n",
    "    :param features_dim: Number of features\n",
    "    :param activation_fn: Activation function\n",
    "    :param normalize_images: Whether to normalize images or not,\n",
    "         dividing by 255.0 (True by default)\n",
    "    :param n_critics: Number of critic networks to create.\n",
    "    :param share_features_extractor: Whether the features extractor is shared or not\n",
    "        between the actor and the critic (this saves computation time)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_space: spaces.Space,\n",
    "        action_space: spaces.Space,\n",
    "        net_arch: List[int],\n",
    "        features_extractor: nn.Module,\n",
    "        features_dim: int,\n",
    "        activation_fn: Type[nn.Module] = nn.ReLU,\n",
    "        normalize_images: bool = True,\n",
    "        n_critics: int = 2,\n",
    "        share_features_extractor: bool = True,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            observation_space,\n",
    "            action_space,\n",
    "            features_extractor=features_extractor,\n",
    "            normalize_images=normalize_images,\n",
    "        )\n",
    "\n",
    "        action_dim = get_action_dim(self.action_space)\n",
    "\n",
    "        self.share_features_extractor = share_features_extractor\n",
    "        self.n_critics = n_critics\n",
    "        self.q_networks = []\n",
    "        for idx in range(n_critics):\n",
    "            q_net = create_mlp(features_dim + action_dim, 1, net_arch, activation_fn)\n",
    "            q_net = nn.Sequential(*q_net)\n",
    "            self.add_module(f\"qf{idx}\", q_net)\n",
    "            self.q_networks.append(q_net)\n",
    "\n",
    "    def forward(self, obs: th.Tensor, actions: th.Tensor) -> Tuple[th.Tensor, ...]:\n",
    "        # Learn the features extractor using the policy loss only\n",
    "        # when the features_extractor is shared with the actor\n",
    "        with th.set_grad_enabled(not self.share_features_extractor):\n",
    "            features = self.extract_features(obs, self.features_extractor)\n",
    "        qvalue_input = th.cat([features, actions], dim=1)\n",
    "        return tuple(q_net(qvalue_input) for q_net in self.q_networks)\n",
    "\n",
    "    def q1_forward(self, obs: th.Tensor, actions: th.Tensor) -> th.Tensor:\n",
    "        \"\"\"\n",
    "        Only predict the Q-value using the first network.\n",
    "        This allows to reduce computation when all the estimates are not needed\n",
    "        (e.g. when updating the policy in TD3).\n",
    "        \"\"\"\n",
    "        with th.no_grad():\n",
    "            features = self.extract_features(obs, self.features_extractor)\n",
    "        return self.q_networks[0](th.cat([features, actions], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "deac0055-f86a-45d7-8076-3aa58b1212dd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from typing import Any, Dict, Optional, Type, TypeVar, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from gym import spaces\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\n",
    "from stable_baselines3.common.policies import ActorCriticCnnPolicy, ActorCriticPolicy, BasePolicy, MultiInputActorCriticPolicy\n",
    "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
    "# from stable_baselines3.common.utils import explained_variance, get_schedule_fn\n",
    "\n",
    "SelfPPO = TypeVar(\"SelfPPO\", bound=\"PPO\")\n",
    "\n",
    "\n",
    "class PPO(OnPolicyAlgorithm):\n",
    "    \"\"\"\n",
    "    Proximal Policy Optimization algorithm (PPO) (clip version)\n",
    "\n",
    "    Paper: https://arxiv.org/abs/1707.06347\n",
    "    Code: This implementation borrows code from OpenAI Spinning Up (https://github.com/openai/spinningup/)\n",
    "    https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail and\n",
    "    Stable Baselines (PPO2 from https://github.com/hill-a/stable-baselines)\n",
    "\n",
    "    Introduction to PPO: https://spinningup.openai.com/en/latest/algorithms/ppo.html\n",
    "\n",
    "    :param policy: The policy model to use (MlpPolicy, CnnPolicy, ...)\n",
    "    :param env: The environment to learn from (if registered in Gym, can be str)\n",
    "    :param learning_rate: The learning rate, it can be a function\n",
    "        of the current progress remaining (from 1 to 0)\n",
    "    :param n_steps: The number of steps to run for each environment per update\n",
    "        (i.e. rollout buffer size is n_steps * n_envs where n_envs is number of environment copies running in parallel)\n",
    "        NOTE: n_steps * n_envs must be greater than 1 (because of the advantage normalization)\n",
    "        See https://github.com/pytorch/pytorch/issues/29372\n",
    "    :param batch_size: Minibatch size\n",
    "    :param n_epochs: Number of epoch when optimizing the surrogate loss\n",
    "    :param gamma: Discount factor\n",
    "    :param gae_lambda: Factor for trade-off of bias vs variance for Generalized Advantage Estimator\n",
    "    :param clip_range: Clipping parameter, it can be a function of the current progress\n",
    "        remaining (from 1 to 0).\n",
    "    :param clip_range_vf: Clipping parameter for the value function,\n",
    "        it can be a function of the current progress remaining (from 1 to 0).\n",
    "        This is a parameter specific to the OpenAI implementation. If None is passed (default),\n",
    "        no clipping will be done on the value function.\n",
    "        IMPORTANT: this clipping depends on the reward scaling.\n",
    "    :param normalize_advantage: Whether to normalize or not the advantage\n",
    "    :param ent_coef: Entropy coefficient for the loss calculation\n",
    "    :param vf_coef: Value function coefficient for the loss calculation\n",
    "    :param max_grad_norm: The maximum value for the gradient clipping\n",
    "    :param use_sde: Whether to use generalized State Dependent Exploration (gSDE)\n",
    "        instead of action noise exploration (default: False)\n",
    "    :param sde_sample_freq: Sample a new noise matrix every n steps when using gSDE\n",
    "        Default: -1 (only sample at the beginning of the rollout)\n",
    "    :param target_kl: Limit the KL divergence between updates,\n",
    "        because the clipping is not enough to prevent large update\n",
    "        see issue #213 (cf https://github.com/hill-a/stable-baselines/issues/213)\n",
    "        By default, there is no limit on the kl div.\n",
    "    :param tensorboard_log: the log location for tensorboard (if None, no logging)\n",
    "    :param policy_kwargs: additional arguments to be passed to the policy on creation\n",
    "    :param verbose: Verbosity level: 0 for no output, 1 for info messages (such as device or wrappers used), 2 for\n",
    "        debug messages\n",
    "    :param seed: Seed for the pseudo random generators\n",
    "    :param device: Device (cpu, cuda, ...) on which the code should be run.\n",
    "        Setting it to auto, the code will be run on the GPU if possible.\n",
    "    :param _init_setup_model: Whether or not to build the network at the creation of the instance\n",
    "    \"\"\"\n",
    "\n",
    "    policy_aliases: Dict[str, Type[BasePolicy]] = {\n",
    "        \"MlpPolicy\": ActorCriticPolicy,\n",
    "        \"CnnPolicy\": ActorCriticCnnPolicy,\n",
    "        \"MultiInputPolicy\": MultiInputActorCriticPolicy,\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        policy: Union[str, Type[ActorCriticPolicy]],\n",
    "        env: Union[GymEnv, str],\n",
    "        learning_rate: Union[float, Schedule] = 3e-4,\n",
    "        n_steps: int = 2048,\n",
    "        batch_size: int = 64,\n",
    "        n_epochs: int = 10,\n",
    "        gamma: float = 0.99,\n",
    "        gae_lambda: float = 0.95,\n",
    "        clip_range: Union[float, Schedule] = 0.2,\n",
    "        clip_range_vf: Union[None, float, Schedule] = None,\n",
    "        normalize_advantage: bool = True,\n",
    "        ent_coef: float = 0.0,\n",
    "        vf_coef: float = 0.5,\n",
    "        max_grad_norm: float = 0.5,\n",
    "        use_sde: bool = False,\n",
    "        sde_sample_freq: int = -1,\n",
    "        target_kl: Optional[float] = None,\n",
    "        tensorboard_log: Optional[str] = None,\n",
    "        policy_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        verbose: int = 0,\n",
    "        seed: Optional[int] = None,\n",
    "        device: Union[th.device, str] = \"auto\",\n",
    "        _init_setup_model: bool = True,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            policy,\n",
    "            env,\n",
    "            learning_rate=learning_rate,\n",
    "            n_steps=n_steps,\n",
    "            gamma=gamma,\n",
    "            gae_lambda=gae_lambda,\n",
    "            ent_coef=ent_coef,\n",
    "            vf_coef=vf_coef,\n",
    "            max_grad_norm=max_grad_norm,\n",
    "            use_sde=use_sde,\n",
    "            sde_sample_freq=sde_sample_freq,\n",
    "            tensorboard_log=tensorboard_log,\n",
    "            policy_kwargs=policy_kwargs,\n",
    "            verbose=verbose,\n",
    "            device=device,\n",
    "            seed=seed,\n",
    "            _init_setup_model=False,\n",
    "            supported_action_spaces=(\n",
    "                spaces.Box,\n",
    "                spaces.Discrete,\n",
    "                spaces.MultiDiscrete,\n",
    "                spaces.MultiBinary,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Sanity check, otherwise it will lead to noisy gradient and NaN\n",
    "        # because of the advantage normalization\n",
    "        if normalize_advantage:\n",
    "            assert (\n",
    "                batch_size > 1\n",
    "            ), \"`batch_size` must be greater than 1. See https://github.com/DLR-RM/stable-baselines3/issues/440\"\n",
    "\n",
    "        if self.env is not None:\n",
    "            # Check that `n_steps * n_envs > 1` to avoid NaN\n",
    "            # when doing advantage normalization\n",
    "            buffer_size = self.env.num_envs * self.n_steps\n",
    "            assert buffer_size > 1 or (\n",
    "                not normalize_advantage\n",
    "            ), f\"`n_steps * n_envs` must be greater than 1. Currently n_steps={self.n_steps} and n_envs={self.env.num_envs}\"\n",
    "            # Check that the rollout buffer size is a multiple of the mini-batch size\n",
    "            untruncated_batches = buffer_size // batch_size\n",
    "            if buffer_size % batch_size > 0:\n",
    "                warnings.warn(\n",
    "                    f\"You have specified a mini-batch size of {batch_size},\"\n",
    "                    f\" but because the `RolloutBuffer` is of size `n_steps * n_envs = {buffer_size}`,\"\n",
    "                    f\" after every {untruncated_batches} untruncated mini-batches,\"\n",
    "                    f\" there will be a truncated mini-batch of size {buffer_size % batch_size}\\n\"\n",
    "                    f\"We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\\n\"\n",
    "                    f\"Info: (n_steps={self.n_steps} and n_envs={self.env.num_envs})\"\n",
    "                )\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "        self.clip_range = clip_range\n",
    "        self.clip_range_vf = clip_range_vf\n",
    "        self.normalize_advantage = normalize_advantage\n",
    "        self.target_kl = target_kl\n",
    "\n",
    "        if _init_setup_model:\n",
    "            self._setup_model()\n",
    "\n",
    "    def _setup_model(self) -> None:\n",
    "        super()._setup_model()\n",
    "\n",
    "        # Initialize schedules for policy/value clipping\n",
    "        self.clip_range = get_schedule_fn(self.clip_range)\n",
    "        if self.clip_range_vf is not None:\n",
    "            if isinstance(self.clip_range_vf, (float, int)):\n",
    "                assert self.clip_range_vf > 0, \"`clip_range_vf` must be positive, \" \"pass `None` to deactivate vf clipping\"\n",
    "\n",
    "            self.clip_range_vf = get_schedule_fn(self.clip_range_vf)\n",
    "\n",
    "    def train(self) -> None:\n",
    "        \"\"\"\n",
    "        Update policy using the currently gathered rollout buffer.\n",
    "        \"\"\"\n",
    "        # Switch to train mode (this affects batch norm / dropout)\n",
    "        self.policy.set_training_mode(True)\n",
    "        # Update optimizer learning rate\n",
    "        self._update_learning_rate(self.policy.optimizer)\n",
    "        # Compute current clip range\n",
    "        clip_range = self.clip_range(self._current_progress_remaining)\n",
    "        # Optional: clip range for the value function\n",
    "        if self.clip_range_vf is not None:\n",
    "            clip_range_vf = self.clip_range_vf(self._current_progress_remaining)\n",
    "\n",
    "        entropy_losses = []\n",
    "        pg_losses, value_losses = [], []\n",
    "        clip_fractions = []\n",
    "\n",
    "        continue_training = True\n",
    "        # train for n_epochs epochs\n",
    "        for epoch in range(self.n_epochs):\n",
    "            approx_kl_divs = []\n",
    "            # Do a complete pass on the rollout buffer\n",
    "            for rollout_data in self.rollout_buffer.get(self.batch_size):\n",
    "                actions = rollout_data.actions\n",
    "                if isinstance(self.action_space, spaces.Discrete):\n",
    "                    # Convert discrete action from float to long\n",
    "                    actions = rollout_data.actions.long().flatten()\n",
    "\n",
    "                # Re-sample the noise matrix because the log_std has changed\n",
    "                if self.use_sde:\n",
    "                    self.policy.reset_noise(self.batch_size)\n",
    "\n",
    "                values, log_prob, entropy = self.policy.evaluate_actions(rollout_data.observations, actions)\n",
    "                values = values.flatten()\n",
    "                # Normalize advantage\n",
    "                advantages = rollout_data.advantages\n",
    "                # Normalization does not make sense if mini batchsize == 1, see GH issue #325\n",
    "                if self.normalize_advantage and len(advantages) > 1:\n",
    "                    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "                # ratio between old and new policy, should be one at the first iteration\n",
    "                ratio = th.exp(log_prob - rollout_data.old_log_prob)\n",
    "\n",
    "                # clipped surrogate loss\n",
    "                policy_loss_1 = advantages * ratio\n",
    "                policy_loss_2 = advantages * th.clamp(ratio, 1 - clip_range, 1 + clip_range)\n",
    "                policy_loss = -th.min(policy_loss_1, policy_loss_2).mean()\n",
    "\n",
    "                # Logging\n",
    "                pg_losses.append(policy_loss.item())\n",
    "                clip_fraction = th.mean((th.abs(ratio - 1) > clip_range).float()).item()\n",
    "                clip_fractions.append(clip_fraction)\n",
    "\n",
    "                if self.clip_range_vf is None:\n",
    "                    # No clipping\n",
    "                    values_pred = values\n",
    "                else:\n",
    "                    # Clip the difference between old and new value\n",
    "                    # NOTE: this depends on the reward scaling\n",
    "                    values_pred = rollout_data.old_values + th.clamp(\n",
    "                        values - rollout_data.old_values, -clip_range_vf, clip_range_vf\n",
    "                    )\n",
    "                # Value loss using the TD(gae_lambda) target\n",
    "                value_loss = F.mse_loss(rollout_data.returns, values_pred)\n",
    "                value_losses.append(value_loss.item())\n",
    "\n",
    "                # Entropy loss favor exploration\n",
    "                if entropy is None:\n",
    "                    # Approximate entropy when no analytical form\n",
    "                    entropy_loss = -th.mean(-log_prob)\n",
    "                else:\n",
    "                    entropy_loss = -th.mean(entropy)\n",
    "\n",
    "                entropy_losses.append(entropy_loss.item())\n",
    "\n",
    "                loss = policy_loss + self.ent_coef * entropy_loss + self.vf_coef * value_loss\n",
    "\n",
    "                # Calculate approximate form of reverse KL Divergence for early stopping\n",
    "                # see issue #417: https://github.com/DLR-RM/stable-baselines3/issues/417\n",
    "                # and discussion in PR #419: https://github.com/DLR-RM/stable-baselines3/pull/419\n",
    "                # and Schulman blog: http://joschu.net/blog/kl-approx.html\n",
    "                with th.no_grad():\n",
    "                    log_ratio = log_prob - rollout_data.old_log_prob\n",
    "                    approx_kl_div = th.mean((th.exp(log_ratio) - 1) - log_ratio).cpu().numpy()\n",
    "                    approx_kl_divs.append(approx_kl_div)\n",
    "\n",
    "                if self.target_kl is not None and approx_kl_div > 1.5 * self.target_kl:\n",
    "                    continue_training = False\n",
    "                    if self.verbose >= 1:\n",
    "                        print(f\"Early stopping at step {epoch} due to reaching max kl: {approx_kl_div:.2f}\")\n",
    "                    break\n",
    "\n",
    "                # Optimization step\n",
    "                self.policy.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # Clip grad norm\n",
    "                th.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
    "                self.policy.optimizer.step()\n",
    "\n",
    "            self._n_updates += 1\n",
    "            if not continue_training:\n",
    "                break\n",
    "\n",
    "        explained_var = explained_variance(self.rollout_buffer.values.flatten(), self.rollout_buffer.returns.flatten())\n",
    "\n",
    "        # Logs\n",
    "        self.logger.record(\"train/entropy_loss\", np.mean(entropy_losses))\n",
    "        self.logger.record(\"train/policy_gradient_loss\", np.mean(pg_losses))\n",
    "        self.logger.record(\"train/value_loss\", np.mean(value_losses))\n",
    "        self.logger.record(\"train/approx_kl\", np.mean(approx_kl_divs))\n",
    "        self.logger.record(\"train/clip_fraction\", np.mean(clip_fractions))\n",
    "        self.logger.record(\"train/loss\", loss.item())\n",
    "        self.logger.record(\"train/explained_variance\", explained_var)\n",
    "        if hasattr(self.policy, \"log_std\"):\n",
    "            self.logger.record(\"train/std\", th.exp(self.policy.log_std).mean().item())\n",
    "\n",
    "        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n",
    "        self.logger.record(\"train/clip_range\", clip_range)\n",
    "        if self.clip_range_vf is not None:\n",
    "            self.logger.record(\"train/clip_range_vf\", clip_range_vf)\n",
    "\n",
    "    def learn(\n",
    "        self: SelfPPO,\n",
    "        total_timesteps: int,\n",
    "        callback: MaybeCallback = None,\n",
    "        log_interval: int = 1,\n",
    "        tb_log_name: str = \"PPO\",\n",
    "        reset_num_timesteps: bool = True,\n",
    "        progress_bar: bool = False,\n",
    "    ) -> SelfPPO:\n",
    "        return super().learn(\n",
    "            total_timesteps=total_timesteps,\n",
    "            callback=callback,\n",
    "            log_interval=log_interval,\n",
    "            tb_log_name=tb_log_name,\n",
    "            reset_num_timesteps=reset_num_timesteps,\n",
    "            progress_bar=progress_bar,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae93884d-3f91-46cf-be6b-ad87a831151e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import platform\n",
    "import random\n",
    "import re\n",
    "from collections import deque\n",
    "from itertools import zip_longest\n",
    "from typing import Dict, Iterable, List, Optional, Tuple, Union\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from gym import spaces\n",
    "\n",
    "import stable_baselines3 as sb3\n",
    "\n",
    "# Check if tensorboard is available for pytorch\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError:\n",
    "    SummaryWriter = None\n",
    "\n",
    "from stable_baselines3.common.logger import Logger, configure\n",
    "from stable_baselines3.common.type_aliases import GymEnv, Schedule, TensorDict, TrainFreq, TrainFrequencyUnit\n",
    "\n",
    "\n",
    "def set_random_seed(seed: int, using_cuda: bool = False) -> None:\n",
    "    \"\"\"\n",
    "    Seed the different random generators.\n",
    "\n",
    "    :param seed:\n",
    "    :param using_cuda:\n",
    "    \"\"\"\n",
    "    # Seed python RNG\n",
    "    random.seed(seed)\n",
    "    # Seed numpy RNG\n",
    "    np.random.seed(seed)\n",
    "    # seed the RNG for all devices (both CPU and CUDA)\n",
    "    th.manual_seed(seed)\n",
    "\n",
    "    if using_cuda:\n",
    "        # Deterministic operations for CuDNN, it may impact performances\n",
    "        th.backends.cudnn.deterministic = True\n",
    "        th.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# From stable baselines\n",
    "def explained_variance(y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes fraction of variance that ypred explains about y.\n",
    "    Returns 1 - Var[y-ypred] / Var[y]\n",
    "\n",
    "    interpretation:\n",
    "        ev=0  =>  might as well have predicted zero\n",
    "        ev=1  =>  perfect prediction\n",
    "        ev<0  =>  worse than just predicting zero\n",
    "\n",
    "    :param y_pred: the prediction\n",
    "    :param y_true: the expected value\n",
    "    :return: explained variance of ypred and y\n",
    "    \"\"\"\n",
    "    assert y_true.ndim == 1 and y_pred.ndim == 1\n",
    "    var_y = np.var(y_true)\n",
    "    return np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "\n",
    "def update_learning_rate(optimizer: th.optim.Optimizer, learning_rate: float) -> None:\n",
    "    \"\"\"\n",
    "    Update the learning rate for a given optimizer.\n",
    "    Useful when doing linear schedule.\n",
    "\n",
    "    :param optimizer: Pytorch optimizer\n",
    "    :param learning_rate: New learning rate value\n",
    "    \"\"\"\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = learning_rate\n",
    "\n",
    "\n",
    "def get_schedule_fn(value_schedule: Union[Schedule, float]) -> Schedule:\n",
    "    \"\"\"\n",
    "    Transform (if needed) learning rate and clip range (for PPO)\n",
    "    to callable.\n",
    "\n",
    "    :param value_schedule: Constant value of schedule function\n",
    "    :return: Schedule function (can return constant value)\n",
    "    \"\"\"\n",
    "    # If the passed schedule is a float\n",
    "    # create a constant function\n",
    "    if isinstance(value_schedule, (float, int)):\n",
    "        # Cast to float to avoid errors\n",
    "        value_schedule = constant_fn(float(value_schedule))\n",
    "    else:\n",
    "        assert callable(value_schedule)\n",
    "    return value_schedule\n",
    "\n",
    "\n",
    "def get_linear_fn(start: float, end: float, end_fraction: float) -> Schedule:\n",
    "    \"\"\"\n",
    "    Create a function that interpolates linearly between start and end\n",
    "    between ``progress_remaining`` = 1 and ``progress_remaining`` = ``end_fraction``.\n",
    "    This is used in DQN for linearly annealing the exploration fraction\n",
    "    (epsilon for the epsilon-greedy strategy).\n",
    "\n",
    "    :params start: value to start with if ``progress_remaining`` = 1\n",
    "    :params end: value to end with if ``progress_remaining`` = 0\n",
    "    :params end_fraction: fraction of ``progress_remaining``\n",
    "        where end is reached e.g 0.1 then end is reached after 10%\n",
    "        of the complete training process.\n",
    "    :return: Linear schedule function.\n",
    "    \"\"\"\n",
    "\n",
    "    def func(progress_remaining: float) -> float:\n",
    "        if (1 - progress_remaining) > end_fraction:\n",
    "            return end\n",
    "        else:\n",
    "            return start + (1 - progress_remaining) * (end - start) / end_fraction\n",
    "\n",
    "    return func\n",
    "\n",
    "\n",
    "def constant_fn(val: float) -> Schedule:\n",
    "    \"\"\"\n",
    "    Create a function that returns a constant\n",
    "    It is useful for learning rate schedule (to avoid code duplication)\n",
    "\n",
    "    :param val: constant value\n",
    "    :return: Constant schedule function.\n",
    "    \"\"\"\n",
    "\n",
    "    def func(_):\n",
    "        return val\n",
    "\n",
    "    return func\n",
    "\n",
    "\n",
    "def get_device(device: Union[th.device, str] = \"auto\") -> th.device:\n",
    "    \"\"\"\n",
    "    Retrieve PyTorch device.\n",
    "    It checks that the requested device is available first.\n",
    "    For now, it supports only cpu and cuda.\n",
    "    By default, it tries to use the gpu.\n",
    "\n",
    "    :param device: One for 'auto', 'cuda', 'cpu'\n",
    "    :return: Supported Pytorch device\n",
    "    \"\"\"\n",
    "    # Cuda by default\n",
    "    if device == \"auto\":\n",
    "        device = \"cuda\"\n",
    "    # Force conversion to th.device\n",
    "    device = th.device(device)\n",
    "\n",
    "    # Cuda not available\n",
    "    if device.type == th.device(\"cuda\").type and not th.cuda.is_available():\n",
    "        return th.device(\"cpu\")\n",
    "\n",
    "    return device\n",
    "\n",
    "\n",
    "def get_latest_run_id(log_path: str = \"\", log_name: str = \"\") -> int:\n",
    "    \"\"\"\n",
    "    Returns the latest run number for the given log name and log path,\n",
    "    by finding the greatest number in the directories.\n",
    "\n",
    "    :param log_path: Path to the log folder containing several runs.\n",
    "    :param log_name: Name of the experiment. Each run is stored\n",
    "        in a folder named ``log_name_1``, ``log_name_2``, ...\n",
    "    :return: latest run number\n",
    "    \"\"\"\n",
    "    max_run_id = 0\n",
    "    for path in glob.glob(os.path.join(log_path, f\"{glob.escape(log_name)}_[0-9]*\")):\n",
    "        file_name = path.split(os.sep)[-1]\n",
    "        ext = file_name.split(\"_\")[-1]\n",
    "        if log_name == \"_\".join(file_name.split(\"_\")[:-1]) and ext.isdigit() and int(ext) > max_run_id:\n",
    "            max_run_id = int(ext)\n",
    "    return max_run_id\n",
    "\n",
    "\n",
    "def configure_logger(\n",
    "    verbose: int = 0,\n",
    "    tensorboard_log: Optional[str] = None,\n",
    "    tb_log_name: str = \"\",\n",
    "    reset_num_timesteps: bool = True,\n",
    ") -> Logger:\n",
    "    \"\"\"\n",
    "    Configure the logger's outputs.\n",
    "\n",
    "    :param verbose: Verbosity level: 0 for no output, 1 for the standard output to be part of the logger outputs\n",
    "    :param tensorboard_log: the log location for tensorboard (if None, no logging)\n",
    "    :param tb_log_name: tensorboard log\n",
    "    :param reset_num_timesteps:  Whether the ``num_timesteps`` attribute is reset or not.\n",
    "        It allows to continue a previous learning curve (``reset_num_timesteps=False``)\n",
    "        or start from t=0 (``reset_num_timesteps=True``, the default).\n",
    "    :return: The logger object\n",
    "    \"\"\"\n",
    "    save_path, format_strings = None, [\"stdout\"]\n",
    "\n",
    "    if tensorboard_log is not None and SummaryWriter is None:\n",
    "        raise ImportError(\"Trying to log data to tensorboard but tensorboard is not installed.\")\n",
    "\n",
    "    if tensorboard_log is not None and SummaryWriter is not None:\n",
    "        latest_run_id = get_latest_run_id(tensorboard_log, tb_log_name)\n",
    "        if not reset_num_timesteps:\n",
    "            # Continue training in the same directory\n",
    "            latest_run_id -= 1\n",
    "        save_path = os.path.join(tensorboard_log, f\"{tb_log_name}_{latest_run_id + 1}\")\n",
    "        if verbose >= 1:\n",
    "            format_strings = [\"stdout\", \"tensorboard\"]\n",
    "        else:\n",
    "            format_strings = [\"tensorboard\"]\n",
    "    elif verbose == 0:\n",
    "        format_strings = [\"\"]\n",
    "    return configure(save_path, format_strings=format_strings)\n",
    "\n",
    "\n",
    "def check_for_correct_spaces(env: GymEnv, observation_space: spaces.Space, action_space: spaces.Space) -> None:\n",
    "    \"\"\"\n",
    "    Checks that the environment has same spaces as provided ones. Used by BaseAlgorithm to check if\n",
    "    spaces match after loading the model with given env.\n",
    "    Checked parameters:\n",
    "    - observation_space\n",
    "    - action_space\n",
    "\n",
    "    :param env: Environment to check for valid spaces\n",
    "    :param observation_space: Observation space to check against\n",
    "    :param action_space: Action space to check against\n",
    "    \"\"\"\n",
    "    if observation_space != env.observation_space:\n",
    "        raise ValueError(f\"Observation spaces do not match: {observation_space} != {env.observation_space}\")\n",
    "    if action_space != env.action_space:\n",
    "        raise ValueError(f\"Action spaces do not match: {action_space} != {env.action_space}\")\n",
    "\n",
    "\n",
    "def check_shape_equal(space1: spaces.Space, space2: spaces.Space) -> None:\n",
    "    \"\"\"\n",
    "    If the spaces are Box, check that they have the same shape.\n",
    "\n",
    "    If the spaces are Dict, it recursively checks the subspaces.\n",
    "\n",
    "    :param space1: Space\n",
    "    :param space2: Other space\n",
    "    \"\"\"\n",
    "    if isinstance(space1, spaces.Dict):\n",
    "        assert isinstance(space2, spaces.Dict), \"spaces must be of the same type\"\n",
    "        assert space1.spaces.keys() == space2.spaces.keys(), \"spaces must have the same keys\"\n",
    "        for key in space1.spaces.keys():\n",
    "            check_shape_equal(space1.spaces[key], space2.spaces[key])\n",
    "    elif isinstance(space1, spaces.Box):\n",
    "        assert space1.shape == space2.shape, \"spaces must have the same shape\"\n",
    "\n",
    "\n",
    "def is_vectorized_box_observation(observation: np.ndarray, observation_space: spaces.Box) -> bool:\n",
    "    \"\"\"\n",
    "    For box observation type, detects and validates the shape,\n",
    "    then returns whether or not the observation is vectorized.\n",
    "\n",
    "    :param observation: the input observation to validate\n",
    "    :param observation_space: the observation space\n",
    "    :return: whether the given observation is vectorized or not\n",
    "    \"\"\"\n",
    "    if observation.shape == observation_space.shape:\n",
    "        return False\n",
    "    elif observation.shape[1:] == observation_space.shape:\n",
    "        return True\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Error: Unexpected observation shape {observation.shape} for \"\n",
    "            + f\"Box environment, please use {observation_space.shape} \"\n",
    "            + \"or (n_env, {}) for the observation shape.\".format(\", \".join(map(str, observation_space.shape)))\n",
    "        )\n",
    "\n",
    "\n",
    "def is_vectorized_discrete_observation(observation: Union[int, np.ndarray], observation_space: spaces.Discrete) -> bool:\n",
    "    \"\"\"\n",
    "    For discrete observation type, detects and validates the shape,\n",
    "    then returns whether or not the observation is vectorized.\n",
    "\n",
    "    :param observation: the input observation to validate\n",
    "    :param observation_space: the observation space\n",
    "    :return: whether the given observation is vectorized or not\n",
    "    \"\"\"\n",
    "    if isinstance(observation, int) or observation.shape == ():  # A numpy array of a number, has shape empty tuple '()'\n",
    "        return False\n",
    "    elif len(observation.shape) == 1:\n",
    "        return True\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Error: Unexpected observation shape {observation.shape} for \"\n",
    "            + \"Discrete environment, please use () or (n_env,) for the observation shape.\"\n",
    "        )\n",
    "\n",
    "\n",
    "def is_vectorized_multidiscrete_observation(observation: np.ndarray, observation_space: spaces.MultiDiscrete) -> bool:\n",
    "    \"\"\"\n",
    "    For multidiscrete observation type, detects and validates the shape,\n",
    "    then returns whether or not the observation is vectorized.\n",
    "\n",
    "    :param observation: the input observation to validate\n",
    "    :param observation_space: the observation space\n",
    "    :return: whether the given observation is vectorized or not\n",
    "    \"\"\"\n",
    "    if observation.shape == (len(observation_space.nvec),):\n",
    "        return False\n",
    "    elif len(observation.shape) == 2 and observation.shape[1] == len(observation_space.nvec):\n",
    "        return True\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Error: Unexpected observation shape {observation.shape} for MultiDiscrete \"\n",
    "            + f\"environment, please use ({len(observation_space.nvec)},) or \"\n",
    "            + f\"(n_env, {len(observation_space.nvec)}) for the observation shape.\"\n",
    "        )\n",
    "\n",
    "\n",
    "def is_vectorized_multibinary_observation(observation: np.ndarray, observation_space: spaces.MultiBinary) -> bool:\n",
    "    \"\"\"\n",
    "    For multibinary observation type, detects and validates the shape,\n",
    "    then returns whether or not the observation is vectorized.\n",
    "\n",
    "    :param observation: the input observation to validate\n",
    "    :param observation_space: the observation space\n",
    "    :return: whether the given observation is vectorized or not\n",
    "    \"\"\"\n",
    "    if observation.shape == observation_space.shape:\n",
    "        return False\n",
    "    elif len(observation.shape) == len(observation_space.shape) + 1 and observation.shape[1:] == observation_space.shape:\n",
    "        return True\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Error: Unexpected observation shape {observation.shape} for MultiBinary \"\n",
    "            + f\"environment, please use {observation_space.shape} or \"\n",
    "            + f\"(n_env, {observation_space.n}) for the observation shape.\"\n",
    "        )\n",
    "\n",
    "\n",
    "def is_vectorized_dict_observation(observation: np.ndarray, observation_space: spaces.Dict) -> bool:\n",
    "    \"\"\"\n",
    "    For dict observation type, detects and validates the shape,\n",
    "    then returns whether or not the observation is vectorized.\n",
    "\n",
    "    :param observation: the input observation to validate\n",
    "    :param observation_space: the observation space\n",
    "    :return: whether the given observation is vectorized or not\n",
    "    \"\"\"\n",
    "    # We first assume that all observations are not vectorized\n",
    "    all_non_vectorized = True\n",
    "    for key, subspace in observation_space.spaces.items():\n",
    "        # This fails when the observation is not vectorized\n",
    "        # or when it has the wrong shape\n",
    "        \n",
    "        ##################################\n",
    "        print(\"KEY: {} - TYPE: {}\"-format(key, type(key)))\n",
    "        ##################################\n",
    "        \n",
    "        if observation[key].shape != subspace.shape:\n",
    "            all_non_vectorized = False\n",
    "            break\n",
    "\n",
    "    if all_non_vectorized:\n",
    "        return False\n",
    "\n",
    "    all_vectorized = True\n",
    "    # Now we check that all observation are vectorized and have the correct shape\n",
    "    for key, subspace in observation_space.spaces.items():\n",
    "        if observation[key].shape[1:] != subspace.shape:\n",
    "            all_vectorized = False\n",
    "            break\n",
    "\n",
    "    if all_vectorized:\n",
    "        return True\n",
    "    else:\n",
    "        # Retrieve error message\n",
    "        error_msg = \"\"\n",
    "        try:\n",
    "            is_vectorized_observation(observation[key], observation_space.spaces[key])\n",
    "        except ValueError as e:\n",
    "            error_msg = f\"{e}\"\n",
    "        raise ValueError(\n",
    "            f\"There seems to be a mix of vectorized and non-vectorized observations. \"\n",
    "            f\"Unexpected observation shape {observation[key].shape} for key {key} \"\n",
    "            f\"of type {observation_space.spaces[key]}. {error_msg}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def is_vectorized_observation(observation: Union[int, np.ndarray], observation_space: spaces.Space) -> bool:\n",
    "    \"\"\"\n",
    "    For every observation type, detects and validates the shape,\n",
    "    then returns whether or not the observation is vectorized.\n",
    "\n",
    "    :param observation: the input observation to validate\n",
    "    :param observation_space: the observation space\n",
    "    :return: whether the given observation is vectorized or not\n",
    "    \"\"\"\n",
    "\n",
    "    is_vec_obs_func_dict = {\n",
    "        spaces.Box: is_vectorized_box_observation,\n",
    "        spaces.Discrete: is_vectorized_discrete_observation,\n",
    "        spaces.MultiDiscrete: is_vectorized_multidiscrete_observation,\n",
    "        spaces.MultiBinary: is_vectorized_multibinary_observation,\n",
    "        spaces.Dict: is_vectorized_dict_observation,\n",
    "    }\n",
    "\n",
    "    for space_type, is_vec_obs_func in is_vec_obs_func_dict.items():\n",
    "        if isinstance(observation_space, space_type):\n",
    "            return is_vec_obs_func(observation, observation_space)\n",
    "    else:\n",
    "        # for-else happens if no break is called\n",
    "        raise ValueError(f\"Error: Cannot determine if the observation is vectorized with the space type {observation_space}.\")\n",
    "\n",
    "\n",
    "def safe_mean(arr: Union[np.ndarray, list, deque]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute the mean of an array if there is at least one element.\n",
    "    For empty array, return NaN. It is used for logging only.\n",
    "\n",
    "    :param arr: Numpy array or list of values\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return np.nan if len(arr) == 0 else np.mean(arr)\n",
    "\n",
    "\n",
    "def get_parameters_by_name(model: th.nn.Module, included_names: Iterable[str]) -> List[th.Tensor]:\n",
    "    \"\"\"\n",
    "    Extract parameters from the state dict of ``model``\n",
    "    if the name contains one of the strings in ``included_names``.\n",
    "\n",
    "    :param model: the model where the parameters come from.\n",
    "    :param included_names: substrings of names to include.\n",
    "    :return: List of parameters values (Pytorch tensors)\n",
    "        that matches the queried names.\n",
    "    \"\"\"\n",
    "    return [param for name, param in model.state_dict().items() if any([key in name for key in included_names])]\n",
    "\n",
    "\n",
    "def zip_strict(*iterables: Iterable) -> Iterable:\n",
    "    r\"\"\"\n",
    "    ``zip()`` function but enforces that iterables are of equal length.\n",
    "    Raises ``ValueError`` if iterables not of equal length.\n",
    "    Code inspired by Stackoverflow answer for question #32954486.\n",
    "\n",
    "    :param \\*iterables: iterables to ``zip()``\n",
    "    \"\"\"\n",
    "    # As in Stackoverflow #32954486, use\n",
    "    # new object for \"empty\" in case we have\n",
    "    # Nones in iterable.\n",
    "    sentinel = object()\n",
    "    for combo in zip_longest(*iterables, fillvalue=sentinel):\n",
    "        if sentinel in combo:\n",
    "            raise ValueError(\"Iterables have different lengths\")\n",
    "        yield combo\n",
    "\n",
    "\n",
    "def polyak_update(\n",
    "    params: Iterable[th.Tensor],\n",
    "    target_params: Iterable[th.Tensor],\n",
    "    tau: float,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Perform a Polyak average update on ``target_params`` using ``params``:\n",
    "    target parameters are slowly updated towards the main parameters.\n",
    "    ``tau``, the soft update coefficient controls the interpolation:\n",
    "    ``tau=1`` corresponds to copying the parameters to the target ones whereas nothing happens when ``tau=0``.\n",
    "    The Polyak update is done in place, with ``no_grad``, and therefore does not create intermediate tensors,\n",
    "    or a computation graph, reducing memory cost and improving performance.  We scale the target params\n",
    "    by ``1-tau`` (in-place), add the new weights, scaled by ``tau`` and store the result of the sum in the target\n",
    "    params (in place).\n",
    "    See https://github.com/DLR-RM/stable-baselines3/issues/93\n",
    "\n",
    "    :param params: parameters to use to update the target params\n",
    "    :param target_params: parameters to update\n",
    "    :param tau: the soft update coefficient (\"Polyak update\", between 0 and 1)\n",
    "    \"\"\"\n",
    "    with th.no_grad():\n",
    "        # zip does not raise an exception if length of parameters does not match.\n",
    "        for param, target_param in zip_strict(params, target_params):\n",
    "            target_param.data.mul_(1 - tau)\n",
    "            th.add(target_param.data, param.data, alpha=tau, out=target_param.data)\n",
    "\n",
    "\n",
    "def obs_as_tensor(\n",
    "    obs: Union[np.ndarray, Dict[Union[str, int], np.ndarray]], device: th.device\n",
    ") -> Union[th.Tensor, TensorDict]:\n",
    "    \"\"\"\n",
    "    Moves the observation to the given device.\n",
    "\n",
    "    :param obs:\n",
    "    :param device: PyTorch device\n",
    "    :return: PyTorch tensor of the observation on a desired device.\n",
    "    \"\"\"\n",
    "    if isinstance(obs, np.ndarray):\n",
    "        return th.as_tensor(obs, device=device)\n",
    "    elif isinstance(obs, dict):\n",
    "        return {key: th.as_tensor(_obs, device=device) for (key, _obs) in obs.items()}\n",
    "    else:\n",
    "        raise Exception(f\"Unrecognized type of observation {type(obs)}\")\n",
    "\n",
    "\n",
    "def should_collect_more_steps(\n",
    "    train_freq: TrainFreq,\n",
    "    num_collected_steps: int,\n",
    "    num_collected_episodes: int,\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Helper used in ``collect_rollouts()`` of off-policy algorithms\n",
    "    to determine the termination condition.\n",
    "\n",
    "    :param train_freq: How much experience should be collected before updating the policy.\n",
    "    :param num_collected_steps: The number of already collected steps.\n",
    "    :param num_collected_episodes: The number of already collected episodes.\n",
    "    :return: Whether to continue or not collecting experience\n",
    "        by doing rollouts of the current policy.\n",
    "    \"\"\"\n",
    "    if train_freq.unit == TrainFrequencyUnit.STEP:\n",
    "        return num_collected_steps < train_freq.frequency\n",
    "\n",
    "    elif train_freq.unit == TrainFrequencyUnit.EPISODE:\n",
    "        return num_collected_episodes < train_freq.frequency\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"The unit of the `train_freq` must be either TrainFrequencyUnit.STEP \"\n",
    "            f\"or TrainFrequencyUnit.EPISODE not '{train_freq.unit}'!\"\n",
    "        )\n",
    "\n",
    "\n",
    "def get_system_info(print_info: bool = True) -> Tuple[Dict[str, str], str]:\n",
    "    \"\"\"\n",
    "    Retrieve system and python env info for the current system.\n",
    "\n",
    "    :param print_info: Whether to print or not those infos\n",
    "    :return: Dictionary summing up the version for each relevant package\n",
    "        and a formatted string.\n",
    "    \"\"\"\n",
    "    env_info = {\n",
    "        # In OS, a regex is used to add a space between a \"#\" and a number to avoid\n",
    "        # wrongly linking to another issue on GitHub. Example: turn \"#42\" to \"# 42\".\n",
    "        \"OS\": re.sub(r\"#(\\d)\", r\"# \\1\", f\"{platform.platform()} {platform.version()}\"),\n",
    "        \"Python\": platform.python_version(),\n",
    "        \"Stable-Baselines3\": sb3.__version__,\n",
    "        \"PyTorch\": th.__version__,\n",
    "        \"GPU Enabled\": str(th.cuda.is_available()),\n",
    "        \"Numpy\": np.__version__,\n",
    "        \"Gym\": gym.__version__,\n",
    "    }\n",
    "    env_info_str = \"\"\n",
    "    for key, value in env_info.items():\n",
    "        env_info_str += f\"- {key}: {value}\\n\"\n",
    "    if print_info:\n",
    "        print(env_info_str)\n",
    "    return env_info, env_info_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14f2d6ea-c4a8-4e84-900a-be9facb9f587",
   "metadata": {},
   "outputs": [],
   "source": [
    "Genv = GridWorldEnv(size= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cda6aa9-a973-419b-a11c-f9f2ae771241",
   "metadata": {},
   "outputs": [],
   "source": [
    "budget_pendulum = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "150857f8-522e-4ad2-bbd5-3d1f0933021c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_model = PPO(\"MultiInputPolicy\", Genv, verbose=0).learn(budget_pendulum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "83082b2f-3895-41f2-9363-a2c7d1c2141a",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = Genv.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "019ae96e-a402-4df9-b3cc-acf9e675580c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'agent': array([2, 9]), 'target': array([5, 1])},\n",
       " ({'agent': array([8, 3]), 'target': array([4, 7])},\n",
       "  -8.0,\n",
       "  False,\n",
       "  False,\n",
       "  {'distance': 8.0}))"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation, Genv.step(Genv.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "2508e317-d477-4352-b202-18b91ffbc8a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Genv.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "cd19b917-b264-4968-aa76-9200c5935a8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1011572107.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [299]\u001b[0;36m\u001b[0m\n\u001b[0;31m    observation = np.ndarray([{'agent': array([7, 2]), 'target': array([8, 1])} {'distance': 2.0}])\u001b[0m\n\u001b[0m                                                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# observation = np.ndarray([{'agent': np.array([7, 2]), 'target': np.array([8, 1])}])\n",
    "observation = np.ndarray([{'agent': array([7, 2]), 'target': array([8, 1])} {'distance': 2.0}])\n",
    "Key = \"agent\"\n",
    "subspace = Box(0, 9, (2,), int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "35aace97-ca4b-461a-86d5-faac4b7ccd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "9573a95d-a8a0-429a-acfc-7550b949bb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_env(Genv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "a92832c7-b9b4-4d1f-a9ca-72c4cfe9e415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = YourEnv()\n",
    "obs = Genv.reset()\n",
    "n_steps = 10\n",
    "for _ in range(n_steps):\n",
    "    # Random action\n",
    "    action = Genv.action_space.sample()\n",
    "    obs, reward, done, truncated, info = Genv.step(action)\n",
    "    if done:\n",
    "        print(\"DONE\")\n",
    "        obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "a54af7fe-d1af-406f-8f91-8d90f992c0d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agent': array([1, 6]), 'target': array([0, 0])}"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Genv._get_obs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d974b30-9442-4367-aae1-255ba4349271",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = Genv.reset()\n",
    "# observation, Genv.step(Genv.action_space.sample())\n",
    "# Genv.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "d26527d5-1eed-41bb-931c-956db21c12c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdic = {\"agent\":[1, 4, 5], \"target\":2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "4753ab9b-f6d2-4523-b353-045721cc6060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('agent', [1, 4, 5]), ('target', 2)])"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdic.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f652cea-0097-47de-9781-5baa29e7a0be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b62ae2-de68-4b85-8a07-79324b22b023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import codecode.interact(local=dict(globals(), **locals()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "ecf17596-1bc8-48e1-8a0f-7fede860ccb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'agent': array([0, 6]), 'target': array([0, 0])}, {'distance': 6.0})"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "041ef76b-a085-46e6-8eec-6ccfd2f65405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space.Space.item: odict_items([('agent', Box(0, 9, (2,), int64)), ('target', Box(0, 9, (2,), int64))])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate tuple (not \"NoneType\") to tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mppo_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/stable_baselines3/common/base_class.py:539\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    521\u001b[0m     observation: Union[np\u001b[38;5;241m.\u001b[39mndarray, Dict[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    524\u001b[0m     deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    525\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, Optional[Tuple[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;124;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/stable_baselines3/common/policies.py:328\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# TODO (GH/1): add support for RNN policies\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# if state is None:\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m#     state = self.initial_state\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# if episode_start is None:\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;66;03m#     episode_start = [False for _ in range(self.n_envs)]\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;66;03m# Switch to eval mode (this affects batch norm / dropout)\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_training_mode(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 328\u001b[0m observation, vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    331\u001b[0m     actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict(observation, deterministic\u001b[38;5;241m=\u001b[39mdeterministic)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/stable_baselines3/common/policies.py:245\u001b[0m, in \u001b[0;36mBaseModel.obs_to_tensor\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    243\u001b[0m     vectorized_env \u001b[38;5;241m=\u001b[39m is_vectorized_observation(observation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space)\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;66;03m# Add batch dimension if needed\u001b[39;00m\n\u001b[0;32m--> 245\u001b[0m     observation \u001b[38;5;241m=\u001b[39m observation\u001b[38;5;241m.\u001b[39mreshape(\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n\u001b[1;32m    247\u001b[0m observation \u001b[38;5;241m=\u001b[39m obs_as_tensor(observation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m observation, vectorized_env\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate tuple (not \"NoneType\") to tuple"
     ]
    }
   ],
   "source": [
    "ppo_model.predict(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "b448f5aa-76e5-42a0-b0f1-8fdbad1392cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = Genv.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "02dcefaf-ad1d-452e-bf6d-73b92a1a56b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Genv._action_to_direction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "ea3b28e1-8e01-4743-ac16-1f97db9718b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: agent | Type: <class 'str'>\n",
      "Observation: [{'agent': array([8, 0]), 'target': array([1, 2])} {'distance': 9.0}] | Type: <class 'numpy.ndarray'>\n",
      "Subspace: Box(0, 9, (2,), int64) | Shape: (2,) | Type: <class 'gym.spaces.box.Box'>\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [258]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     action, _states \u001b[38;5;241m=\u001b[39m \u001b[43mppo_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAction: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(action))\n\u001b[1;32m      4\u001b[0m     obs, rewards, dones, truncated, info \u001b[38;5;241m=\u001b[39m Genv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/stable_baselines3/common/base_class.py:539\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    521\u001b[0m     observation: Union[np\u001b[38;5;241m.\u001b[39mndarray, Dict[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    524\u001b[0m     deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    525\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, Optional[Tuple[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;124;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/stable_baselines3/common/policies.py:328\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# TODO (GH/1): add support for RNN policies\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# if state is None:\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m#     state = self.initial_state\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# if episode_start is None:\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;66;03m#     episode_start = [False for _ in range(self.n_envs)]\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;66;03m# Switch to eval mode (this affects batch norm / dropout)\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_training_mode(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 328\u001b[0m observation, vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    331\u001b[0m     actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict(observation, deterministic\u001b[38;5;241m=\u001b[39mdeterministic)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/stable_baselines3/common/policies.py:243\u001b[0m, in \u001b[0;36mBaseModel.obs_to_tensor\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    239\u001b[0m     observation \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(observation)\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m# Dict obs need to be handled separately\u001b[39;00m\n\u001b[0;32m--> 243\u001b[0m     vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[43mis_vectorized_observation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;66;03m# Add batch dimension if needed\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     observation \u001b[38;5;241m=\u001b[39m observation\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/stable_baselines3/common/utils.py:409\u001b[0m, in \u001b[0;36mis_vectorized_observation\u001b[0;34m(observation, observation_space)\u001b[0m\n\u001b[1;32m    407\u001b[0m \n\u001b[1;32m    408\u001b[0m \n\u001b[0;32m--> 409\u001b[0m def safe_mean(arr: Union[np.ndarray, list, deque]) -> np.ndarray:\n\u001b[1;32m    410\u001b[0m     \"\"\"\n\u001b[1;32m    411\u001b[0m     Compute the mean of an array if there is at least one element.\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/stable_baselines3/common/utils.py:354\u001b[0m, in \u001b[0;36mis_vectorized_dict_observation\u001b[0;34m(observation, observation_space)\u001b[0m\n\u001b[1;32m    352\u001b[0m \n\u001b[1;32m    353\u001b[0m     if all_non_vectorized:\n\u001b[0;32m--> 354\u001b[0m         return False\n\u001b[1;32m    355\u001b[0m \n\u001b[1;32m    356\u001b[0m     all_vectorized = True\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    action, _states = ppo_model.predict(obs)\n",
    "    print(\"Action: {}\".format(action))\n",
    "    obs, rewards, dones, truncated, info = Genv.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "f2ed579d-53cf-48b6-bf43-7bd89b8345d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [219]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m t\u001b[38;5;241m=\u001b[39m\u001b[43mGenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [110]\u001b[0m, in \u001b[0;36mGridWorldEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# Map the action (element of {0,1,2,3}) to the direction we walk in\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m     direction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_action_to_direction\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# We use `np.clip` to make sure we don't leave the grid\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agent_location \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agent_location \u001b[38;5;241m+\u001b[39m direction, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     79\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "t=Genv.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "42a5da6a-da27-42ed-8371-049828a65e54",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [218]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m a, b, c, d, e \u001b[38;5;241m=\u001b[39m \u001b[43mGenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [110]\u001b[0m, in \u001b[0;36mGridWorldEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# Map the action (element of {0,1,2,3}) to the direction we walk in\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m     direction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_action_to_direction\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# We use `np.clip` to make sure we don't leave the grid\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agent_location \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agent_location \u001b[38;5;241m+\u001b[39m direction, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     79\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "a, b, c, d, e = Genv.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "c0393ff2-be9c-4d66-884f-24465f4dc891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cf54e2-63d1-4def-9612-ac5797301594",
   "metadata": {},
   "source": [
    "# 3 Shower Example\n",
    "based on the video: https://www.youtube.com/watch?v=bD6V3rcr_54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bdd0a0-1f03-4608-a543-6f155dde1418",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewEnv(Env):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # actions\n",
    "        self.action_space = Discrete(3)\n",
    "        # state\n",
    "        self.state = 38 + random.randint(-3,3)\n",
    "        # observation\n",
    "        self.observation_space = Box(low = np.array([0]), high = np.array([100]))\n",
    "        # episodes\n",
    "        self.episodes_length = 60\n",
    "        \n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        self.state += action - 1\n",
    "        self.episodes_length -= 1\n",
    "        if self.state >=37 and self.state <= 39:\n",
    "            reward += 1\n",
    "        else:\n",
    "            reward -= 1\n",
    "        # check if the time is up \n",
    "        if self.episodes_length <= 0:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        # Apply temperature noise\n",
    "        self.state += random.randint(-1,1)\n",
    "        \n",
    "        # placeholder for info\n",
    "        info = spaces.Box(low=-1.0, high=2.0, shape=(3, 4), dtype=np.float32)\n",
    "        \n",
    "        return self.state, reward, done, info\n",
    "        \n",
    "    def render(self):\n",
    "        # implement visualization\n",
    "        pass\n",
    "    def reset(self):\n",
    "        # reset\n",
    "        self.state = 38 + random.randint(-3,3)\n",
    "        self.episodes_length = 60\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eaf8ed9f-df8d-4a1a-8ed8-b843cf0c7b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = NewEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c136ec05-ab57-4a37-b470-cce196eb70dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([60.771236], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a293cc2c-3f76-465e-89c0-7eba47d81553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41,\n",
       " -1,\n",
       " False,\n",
       " Box([[-1. -1. -1. -1.]\n",
       "  [-1. -1. -1. -1.]\n",
       "  [-1. -1. -1. -1.]], [[2. 2. 2. 2.]\n",
       "  [2. 2. 2. 2.]\n",
       "  [2. 2. 2. 2.]], (3, 4), float32))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6275c5a7-0d78-4461-8292-e153e7545c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 | State: 35 | Action: 0 | Score: 2\n",
      "Episode: 1 | State: 32 | Action: 2 | Score: -54\n",
      "Episode: 2 | State: 36 | Action: 1 | Score: 4\n",
      "Episode: 3 | State: 41 | Action: 1 | Score: 10\n",
      "Episode: 4 | State: 30 | Action: 0 | Score: -30\n",
      "Episode: 5 | State: 30 | Action: 2 | Score: -52\n",
      "Episode: 6 | State: 38 | Action: 1 | Score: -36\n",
      "Episode: 7 | State: 39 | Action: 2 | Score: -34\n",
      "Episode: 8 | State: 36 | Action: 1 | Score: -38\n",
      "Episode: 9 | State: 39 | Action: 2 | Score: -30\n"
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    print('Episode: {} | State: {} | Action: {} | Score: {}'.format(episode, n_state, action, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4ebd13-c6a2-406b-8082-e59be728a7ae",
   "metadata": {},
   "source": [
    "## RL on \"shower environment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f982edc-fa29-45fd-849e-3eb6208c9192",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO, A2C, SAC, TD3, DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e7ed3e04-f1d5-4f7a-bf7b-63f20ee61e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install stable_baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6523fd3b-089f-4c95-bb32-a7f3d8f41a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4000 training timesteps\n",
    "budget_pendulum = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c209b3dc-ffd0-44dd-b226-5fc4b6182d68",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Box' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ppo_model \u001b[38;5;241m=\u001b[39m \u001b[43mPPO\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMlpPolicy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbudget_pendulum\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:307\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    299\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    304\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    305\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py:248\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    244\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 248\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m continue_training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py:184\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callback\u001b[38;5;241m.\u001b[39mon_step() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_info_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m n_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space, spaces\u001b[38;5;241m.\u001b[39mDiscrete):\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;66;03m# Reshape in case of discrete action\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/stable_baselines3/common/base_class.py:434\u001b[0m, in \u001b[0;36mBaseAlgorithm._update_info_buffer\u001b[0;34m(self, infos, dones)\u001b[0m\n\u001b[1;32m    432\u001b[0m     dones \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28;01mFalse\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(infos))\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, info \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(infos):\n\u001b[0;32m--> 434\u001b[0m     maybe_ep_info \u001b[38;5;241m=\u001b[39m \u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    435\u001b[0m     maybe_is_success \u001b[38;5;241m=\u001b[39m info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_success\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m maybe_ep_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Box' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "ppo_model = PPO(\"MlpPolicy\", env, verbose=0).learn(budget_pendulum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c86744-3a78-48fa-a1a2-a464999a22ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_model.learn(total_timesteps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba79fad-b308-4e57-9d8a-7faf1a5a3715",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00f2109-299a-448d-93b0-6f14ddb3d77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    action, _states = ppo_model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bb72c9b5-a4b6-4eb5-89e4-5ddb095ce392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37, 1, False, {})"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "183fa867-b692-4a5a-b464-54ef3c38271d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Box2D'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [103]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLunarLander-v2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhuman\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      5\u001b[0m observation, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/gym/envs/registration.py:581\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    578\u001b[0m     env_creator \u001b[38;5;241m=\u001b[39m spec_\u001b[38;5;241m.\u001b[39mentry_point\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;66;03m# Assume it's a string\u001b[39;00m\n\u001b[0;32m--> 581\u001b[0m     env_creator \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentry_point\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    583\u001b[0m mode \u001b[38;5;241m=\u001b[39m _kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrender_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    584\u001b[0m apply_human_rendering \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/gym/envs/registration.py:61\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m\"\"\"Loads an environment with name and returns an environment creation function\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m    Calls the environment constructor\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     60\u001b[0m mod_name, attr_name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 61\u001b[0m mod \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(mod, attr_name)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:850\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:228\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/gym/envs/box2d/lunar_lander.py:33\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mBox2D\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mBox2D\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mb2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     35\u001b[0m     edgeShape,\n\u001b[1;32m     36\u001b[0m     circleShape,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m     contactListener,\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Box2D'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "env.action_space.seed(42)\n",
    "\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "for _ in range(1000):\n",
    "    observation, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
